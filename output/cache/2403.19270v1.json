{
  "id": "2403.19270v1",
  "title": "sDPO: Don't Use Your Data All at Once",
  "pdf_url": "http://arxiv.org/pdf/2403.19270v1",
  "raw_tldr": "动机\t随着大型语言模型的发展，将它们与人类偏好相结合变得越来越重要。\n方法\t本文提出了逐步直接偏好优化（stepwise DPO，简称sDPO），这是最近流行起来的直接偏好优化（DPO）的扩展，用于调整对齐。这种方法涉及将可用的偏好数据集分割开来，并以逐步的方式使用它们，而不是一次性全部使用。\n结果\t本研究表明，这种方法促进了在DPO训练框架内使用更精确对齐的参考模型。此外，sDPO训练的最终模型性能更佳，甚至超过了其他参数更多的流行大型语言模型。",
  "tldr": {
    "动机": "随着大型语言模型的发展，将它们与人类偏好相结合变得越来越重要。",
    "方法": "本文提出了逐步直接偏好优化（stepwise DPO，简称sDPO），这是最近流行起来的直接偏好优化（DPO）的扩展，用于调整对齐。这种方法涉及将可用的偏好数据集分割开来，并以逐步的方式使用它们，而不是一次性全部使用。",
    "结果": "本研究表明，这种方法促进了在DPO训练框架内使用更精确对齐的参考模型。此外，sDPO训练的最终模型性能更佳，甚至超过了其他参数更多的流行大型语言模型。"
  },
  "summary_cn": "随着大型语言模型（LLM）的发展，将它们与人类偏好相一致变得越来越重要。我们提出了逐步直接偏好优化（sDPO），这是最近流行化的直接偏好优化（DPO）用于调整对齐的一种扩展。这种方法涉及将可用的偏好数据集分割开来，并以逐步的方式使用它们，而不是一次性全部使用。我们证明，这种方法有助于在DPO训练框架内使用更精确对齐的参考模型。此外，sDPO训练的最终模型表现更佳，甚至超过了其他具有更多参数的流行大型语言模型。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"Machine Learning\",\n  \"标签\": [\"大型语言模型\", \"人类偏好对齐\", \"偏好优化\", \"训练框架\", \"性能提升\"]\n}\n```",
  "tag_info": {
    "主要领域": "Machine Learning",
    "标签": [
      "大型语言模型",
      "人类偏好对齐",
      "偏好优化",
      "训练框架",
      "性能提升"
    ]
  }
}
{
  "id": "2403.09629v1",
  "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking",
  "pdf_url": "http://arxiv.org/pdf/2403.09629v1",
  "raw_tldr": "动机\t人们在写作和交谈时有时会暂停思考。尽管以往的以推理为中心的工作通常将推理视为回答问题或完成主动任务的方法，但几乎所有书面文本中都隐含着推理过程。因此，存在一种需求，即开发一种方法，使语言模型能够在任意文本中推断未明确陈述的推理过程。\n方法\t我们提出了Quiet-STaR，这是STaR的一个泛化版本，其中语言模型（LM）学习在每个令牌处生成理由来解释未来的文本，从而改进它们的预测。为了解决生成续写的计算成本、LM最初不知道如何生成或使用内部思考以及需要预测超出单个下一个令牌的需求等关键挑战，我们提出了一种基于令牌的并行采样算法，使用可学习的令牌指示思考的开始和结束，并采用扩展的教师强制技术。\n结果\t在使用Quiet-STaR对一个互联网文本语料库进行持续预训练后，我们发现在GSM8K（从5.9%提高到10.9%）和CommonsenseQA（从36.3%提高到47.2%）上实现了零样本改进，并观察到自然文本中难以预测令牌的困惑度改进。重要的是，这些改进不需要对这些任务进行微调。",
  "tldr": {
    "动机": "人们在写作和交谈时有时会暂停思考。尽管以往的以推理为中心的工作通常将推理视为回答问题或完成主动任务的方法，但几乎所有书面文本中都隐含着推理过程。因此，存在一种需求，即开发一种方法，使语言模型能够在任意文本中推断未明确陈述的推理过程。",
    "方法": "我们提出了Quiet-STaR，这是STaR的一个泛化版本，其中语言模型（LM）学习在每个令牌处生成理由来解释未来的文本，从而改进它们的预测。为了解决生成续写的计算成本、LM最初不知道如何生成或使用内部思考以及需要预测超出单个下一个令牌的需求等关键挑战，我们提出了一种基于令牌的并行采样算法，使用可学习的令牌指示思考的开始和结束，并采用扩展的教师强制技术。",
    "结果": "在使用Quiet-STaR对一个互联网文本语料库进行持续预训练后，我们发现在GSM8K（从5.9%提高到10.9%）和CommonsenseQA（从36.3%提高到47.2%）上实现了零样本改进，并观察到自然文本中难以预测令牌的困惑度改进。重要的是，这些改进不需要对这些任务进行微调。"
  },
  "summary_cn": "在写作和交谈时，人们有时会暂停思考。尽管以推理为中心的作品经常将推理框定为回答问题或完成主体任务的一种方法，但几乎所有书面文本中都隐含着推理。例如，这适用于证明中未陈述的步骤，或是对话背后的心理理论。在自学推理者（STaR，Zelikman等人，2022年）中，通过从少量示例中推断出问题回答的理由，并从那些能够得出正确答案的示例中学习，来学习有用的思考。这是一个高度受限的设置——理想情况下，语言模型可以学会从任意文本中推断未陈述的理由。我们提出了Quiet-STaR，它是STaR的一个泛化版本，其中语言模型（LM）学会在每个标记处生成理由以解释未来的文本，从而改进它们的预测。我们解决了关键挑战，包括1)生成延续的计算成本，2)语言模型最初不知道如何生成或使用内部思想，以及3)需要预测超出单个下一个标记的需求。为了解决这些问题，我们提出了一种基于标记的并行采样算法，使用可学习的标记来指示思想的开始和结束，以及一种扩展的强制教学技术。令人鼓舞的是，生成的理由对于帮助模型预测难以预测的标记有着不成比例的帮助，并提高了LM直接回答困难问题的能力。特别是，在对一个互联网文本语料库进行了Quiet-STaR的持续预训练后，我们发现在GSM8K（从5.9%提高到10.9%）和CommonsenseQA（从36.3%提高到47.2%）上的零次射击改进，并观察到自然文本中难以预测的标记的困惑度改进。关键的是，这些改进不需要对这些任务进行微调。Quiet-STaR标志着向能够以更通用和可扩展的方式学习推理的语言模型迈出的一步。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"自然语言处理\",\n    \"推理\",\n    \"语言模型\",\n    \"少样本学习\",\n    \"生成模型\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "自然语言处理",
      "推理",
      "语言模型",
      "少样本学习",
      "生成模型"
    ]
  }
}
{
  "id": "2402.10790v2",
  "title": "In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss",
  "pdf_url": "http://arxiv.org/pdf/2402.10790v2",
  "raw_tldr": "动机\t处理长文档对于生成式变换模型来说是一个挑战。\n方法\t通过引入一个新的基准测试BABILong来评估不同方法，并对GPT-4和RAG进行了基准测试。\n结果\t通过对GPT-2进行微调并增加循环记忆，使其能够处理高达$11\\times 10^6$元素的任务，这是迄今为止任何神经网络模型处理的最长输入，显著提高了处理长序列的能力。",
  "tldr": {
    "动机": "处理长文档对于生成式变换模型来说是一个挑战。",
    "方法": "通过引入一个新的基准测试BABILong来评估不同方法，并对GPT-4和RAG进行了基准测试。",
    "结果": "通过对GPT-2进行微调并增加循环记忆，使其能够处理高达$11\times 10^6$元素的任务，这是迄今为止任何神经网络模型处理的最长输入，显著提高了处理长序列的能力。"
  },
  "summary_cn": "本文解决了使用生成式变换模型处理长文档的挑战。为了评估不同的方法，我们引入了BABILong，这是一个新的基准测试，旨在评估模型在提取和处理大量文本中分布式事实的能力。我们的评估包括了对GPT-4和RAG的基准测试，结果显示常用方法仅对长度达到$10^4$元素的序列有效。相比之下，通过对GPT-2进行微调并增加循环记忆，使其能够处理涉及多达$11\\times 10^6$元素的任务。这一成就标志着一个重大飞跃，因为这是迄今为止任何神经网络模型处理的最长输入，展示了在处理长序列方面的显著改进。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\"长文本处理\", \"生成式变换模型\", \"GPT-4\", \"RAG\", \"GPT-2\", \"循环记忆增强\", \"BABILong\"]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "长文本处理",
      "生成式变换模型",
      "GPT-4",
      "RAG",
      "GPT-2",
      "循环记忆增强",
      "BABILong"
    ]
  }
}
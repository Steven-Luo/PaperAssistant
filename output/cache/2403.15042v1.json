{
  "id": "2403.15042v1",
  "title": "LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement",
  "pdf_url": "http://arxiv.org/pdf/2403.15042v1",
  "raw_tldr": "动机：为了解决在低数据环境下预训练大型语言模型（LLMs）进行微调的挑战，提高在自然语言处理任务中的性能。\n方法：提出了LLM2LLM，一种针对性的迭代数据增强策略，使用教师LLM通过增加额外数据来增强小型种子数据集，用于特定任务的微调。该策略包括：微调基线学生LLM，评估并提取模型预测错误的数据点，使用教师LLM基于这些错误数据点生成合成数据，并将其重新加入训练数据中。\n结果：LLM2LLM显著提升了LLMs在低数据环境下的性能，超越了传统的微调和其他数据增强基线。在不同数据集上，相比于常规微调，LLM2LLM实现了高达24.2%（GSM8K）、32.6%（CaseHOLD）、32.0%（SNIPS）、52.6%（TREC）和39.8%（SST-2）的性能提升。",
  "tldr": {
    "动机：为了解决在低数据环境下预训练大型语言模型（LLMs）进行微调的挑战，提高在自然语言处理任务中的性能。": "",
    "方法：提出了LLM2LLM，一种针对性的迭代数据增强策略，使用教师LLM通过增加额外数据来增强小型种子数据集，用于特定任务的微调。该策略包括：微调基线学生LLM，评估并提取模型预测错误的数据点，使用教师LLM基于这些错误数据点生成合成数据，并将其重新加入训练数据中。": "",
    "结果：LLM2LLM显著提升了LLMs在低数据环境下的性能，超越了传统的微调和其他数据增强基线。在不同数据集上，相比于常规微调，LLM2LLM实现了高达24.2%（GSM8K）、32.6%（CaseHOLD）、32.0%（SNIPS）、52.6%（TREC）和39.8%（SST-2）的性能提升。": "",
    "动机": "",
    "方法": "",
    "结果": ""
  },
  "summary_cn": "预训练的大型语言模型（LLMs）目前是解决绝大多数自然语言处理任务的最先进方法。尽管许多实际应用仍需进行微调以达到令人满意的性能水平，但许多应用处于数据匮乏的状态，使得微调变得具有挑战性。为了解决这一问题，我们提出了LLM2LLM，这是一种针对性的、迭代的数据增强策略，它使用一个教师LLM通过增加可用于特定任务微调的数据来增强一个小型种子数据集。LLM2LLM (1) 在初始种子数据上对基线学生LLM进行微调，(2) 评估并提取模型判断错误的数据点，以及 (3) 使用教师LLM基于这些错误数据点生成合成数据，然后将这些数据重新添加到训练数据中。这种方法通过在训练期间放大LLM错误预测数据点的信号，并将其重新整合到数据集中，从而专注于对LLM来说更具挑战性的例子。我们的结果表明，LLM2LLM显著提高了LLM在数据匮乏状态下的性能，超越了传统的微调和其他数据增强基线。LLM2LLM减少了对劳动密集型数据策划的依赖，并为更可扩展和高性能的LLM解决方案铺平了道路，使我们能够处理数据受限的领域和任务。我们在使用LLaMA2-7B学生模型的低数据状态下，相对于常规微调，在GSM8K数据集上取得了高达24.2%的改进，在CaseHOLD上为32.6%，在SNIPS上为32.0%，在TREC上为52.6%，在SST-2上为39.8%。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"自然语言处理 (NLP)\",\n  \"标签\": [\n    \"预训练语言模型 (Pretrained Language Models)\",\n    \"数据增强 (Data Augmentation)\",\n    \"低数据环境 (Low-Data Regime)\",\n    \"微调 (Fine-Tuning)\",\n    \"合成数据生成 (Synthetic Data Generation)\",\n    \"模型性能提升 (Model Performance Enhancement)\",\n    \"数据集 (Datasets)\"\n  ],\n  \"方法\": \"LLM2LLM\",\n  \"结果\": {\n    \"GSM8K\": \"提升 24.2%\",\n    \"CaseHOLD\": \"提升 32.6%\",\n    \"SNIPS\": \"提升 32.0%\",\n    \"TREC\": \"提升 52.6%\",\n    \"SST-2\": \"提升 39.8%\"\n  },\n  \"模型\": \"LLaMA2-7B\"\n}\n```",
  "tag_info": {
    "主要领域": "自然语言处理 (NLP)",
    "标签": [
      "预训练语言模型 (Pretrained Language Models)",
      "数据增强 (Data Augmentation)",
      "低数据环境 (Low-Data Regime)",
      "微调 (Fine-Tuning)",
      "合成数据生成 (Synthetic Data Generation)",
      "模型性能提升 (Model Performance Enhancement)",
      "数据集 (Datasets)"
    ],
    "方法": "LLM2LLM",
    "结果": {
      "GSM8K": "提升 24.2%",
      "CaseHOLD": "提升 32.6%",
      "SNIPS": "提升 32.0%",
      "TREC": "提升 52.6%",
      "SST-2": "提升 39.8%"
    },
    "模型": "LLaMA2-7B"
  }
}
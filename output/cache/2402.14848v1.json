{
  "id": "2402.14848v1",
  "raw_tldr": "动机\t本文探讨了增加输入长度对大型语言模型（LLMs）能力的影响，因为目前对LLMs在不同输入长度下的性能一致性了解不足。\n方法\t通过引入一个新颖的QA推理框架，专门设计来评估输入长度的影响，并通过使用不同长度、类型和位置的填充来扩展同一样本的多个版本，从而隔离输入长度的效果。\n结果\t研究发现LLMs在远低于其技术最大值的输入长度上就表现出明显的推理性能下降，且这种下降趋势在数据集的每个版本中都存在，尽管强度不同。此外，研究揭示传统的困惑度指标与LLMs在长输入推理任务中的性能不相关，并识别了可以为未来研究提供有用指导的失败模式，有助于制定策略解决在LLMs中观察到的限制。",
  "tldr": {
    "动机": "本文探讨了增加输入长度对大型语言模型（LLMs）能力的影响，因为目前对LLMs在不同输入长度下的性能一致性了解不足。",
    "方法": "通过引入一个新颖的QA推理框架，专门设计来评估输入长度的影响，并通过使用不同长度、类型和位置的填充来扩展同一样本的多个版本，从而隔离输入长度的效果。",
    "结果": "研究发现LLMs在远低于其技术最大值的输入长度上就表现出明显的推理性能下降，且这种下降趋势在数据集的每个版本中都存在，尽管强度不同。此外，研究揭示传统的困惑度指标与LLMs在长输入推理任务中的性能不相关，并识别了可以为未来研究提供有用指导的失败模式，有助于制定策略解决在LLMs中观察到的限制。"
  },
  "summary_cn": "本文探讨了延长输入长度对大型语言模型（LLMs）能力的影响。尽管近来LLMs取得了进步，但它们在不同输入长度上的性能一致性尚不清楚。我们通过引入一个新颖的问答推理框架来研究这一方面，该框架专门设计用于评估输入长度的影响。我们通过使用不同长度、类型和位置的填充来扩展同一样本的多个版本，从而隔离输入长度的效果。我们的发现显示，LLMs在远低于其技术最大值的输入长度上的推理性能出现了显著下降。我们展示了这种退化趋势出现在我们数据集的每个版本中，尽管强度不同。此外，我们的研究揭示，传统的困惑度指标与LLMs在长输入推理任务中的性能不相关。我们分析了我们的结果，并识别了可以作为未来研究有用指南的失败模式，这可能有助于制定策略来解决在LLMs中观察到的限制。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"Large Language Models\",\n    \"Input Length\",\n    \"QA Reasoning Framework\",\n    \"Performance Degradation\",\n    \"Perplexity Metrics\",\n    \"Failure Modes\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "Large Language Models",
      "Input Length",
      "QA Reasoning Framework",
      "Performance Degradation",
      "Perplexity Metrics",
      "Failure Modes"
    ]
  }
}
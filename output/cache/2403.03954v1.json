{
  "id": "2403.03954v1",
  "title": "3D Diffusion Policy",
  "pdf_url": "http://arxiv.org/pdf/2403.03954v1",
  "raw_tldr": "动机\t模仿学习是教授机器人灵巧技能的有效方式，但是学习复杂技能通常需要大量的人类示范，这既耗时又难以实现技能的鲁棒性和泛化性。\n方法\t提出了一种新颖的视觉模仿学习方法——3D Diffusion Policy (DP3)，它将3D视觉表示的力量整合到扩散策略中，这是一类条件动作生成模型。DP3的核心设计是利用一个从稀疏点云中提取的紧凑3D视觉表示，通过一个高效的点编码器实现。\n结果\t在72个模拟任务的实验中，DP3仅使用10个示范就成功处理了大多数任务，并且相比基线方法实现了55.3%的相对改进。在4个真实机器人任务中，DP3仅使用每个任务40个示范就展示了精确的控制和85%的高成功率，并且在空间、视点、外观和实例等多个方面展现了出色的泛化能力。在真实机器人实验中，DP3很少违反安全要求，与经常需要人为干预的基线方法形成鲜明对比。",
  "tldr": {
    "动机": "模仿学习是教授机器人灵巧技能的有效方式，但是学习复杂技能通常需要大量的人类示范，这既耗时又难以实现技能的鲁棒性和泛化性。",
    "方法": "提出了一种新颖的视觉模仿学习方法——3D Diffusion Policy (DP3)，它将3D视觉表示的力量整合到扩散策略中，这是一类条件动作生成模型。DP3的核心设计是利用一个从稀疏点云中提取的紧凑3D视觉表示，通过一个高效的点编码器实现。",
    "结果": "在72个模拟任务的实验中，DP3仅使用10个示范就成功处理了大多数任务，并且相比基线方法实现了55.3%的相对改进。在4个真实机器人任务中，DP3仅使用每个任务40个示范就展示了精确的控制和85%的高成功率，并且在空间、视点、外观和实例等多个方面展现了出色的泛化能力。在真实机器人实验中，DP3很少违反安全要求，与经常需要人为干预的基线方法形成鲜明对比。"
  },
  "summary_cn": "模仿学习为教授机器人灵巧技能提供了一种高效的方式；然而，稳健且具有泛化能力地学习复杂技能通常需要大量的人类示范。为了解决这一挑战性问题，我们提出了一种新颖的视觉模仿学习方法——3D扩散策略（DP3），它将3D视觉表示的力量融入到扩散策略中，后者是一类条件动作生成模型。DP3的核心设计是利用一个紧凑的3D视觉表示，该表示从稀疏点云中通过一个高效的点编码器提取。在我们涉及72个仿真任务的实验中，DP3仅使用10个示范就成功处理了大多数任务，并且相对于基线方法有55.3%的相对改进。在4个真实机器人任务中，DP3展示了精确的控制能力，仅给定每个任务40个示范就达到了85%的高成功率，并且在包括空间、视点、外观和实例在内的多个方面显示出了优秀的泛化能力。有趣的是，在真实机器人实验中，DP3很少违反安全要求，与经常需要人为干预的基线方法形成鲜明对比。我们的广泛评估突显了3D表示在现实世界机器人学习中的关键重要性。视频、代码和数据可在 https://3d-diffusion-policy.github.io 上获得。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"Robotics\",\n  \"标签\": [\n    \"Imitation Learning\",\n    \"3D Visual Representations\",\n    \"Diffusion Policies\",\n    \"Robotics\",\n    \"Point Clouds\",\n    \"Generalization\",\n    \"Safety\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "Robotics",
    "标签": [
      "Imitation Learning",
      "3D Visual Representations",
      "Diffusion Policies",
      "Robotics",
      "Point Clouds",
      "Generalization",
      "Safety"
    ]
  }
}
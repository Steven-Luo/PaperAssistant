{
  "id": "2402.17485v1",
  "raw_tldr": "动机\t\\t本研究旨在通过关注音频线索与面部动作之间的动态和细腻关系，解决提升说话头视频生成中真实感和表现力的挑战。\n方法\t\\t提出了一种名为EMO的新框架，采用直接音频到视频的合成方法，避免了中间3D模型或面部标志的需要，确保了视频中无缝帧过渡和一致的身份保持。\n结果\t\\tEMO不仅能生成令人信服的说话视频，还能生成各种风格的唱歌视频，在表现力和真实感方面显著超越现有的最先进方法。",
  "tldr": {
    "动机": "\\t本研究旨在通过关注音频线索与面部动作之间的动态和细腻关系，解决提升说话头视频生成中真实感和表现力的挑战。",
    "方法": "\\t提出了一种名为EMO的新框架，采用直接音频到视频的合成方法，避免了中间3D模型或面部标志的需要，确保了视频中无缝帧过渡和一致的身份保持。",
    "结果": "\\tEMO不仅能生成令人信服的说话视频，还能生成各种风格的唱歌视频，在表现力和真实感方面显著超越现有的最先进方法。"
  },
  "summary_cn": "在这项工作中，我们着手解决提升说话头像视频生成中真实感和表现力的挑战，重点关注音频线索与面部动作之间的动态细腻关系。我们发现传统技术的局限性，这些技术常常未能捕捉到人类表情的全谱和个体面部风格的独特性。为了解决这些问题，我们提出了EMO，一个新颖的框架，采用直接音频到视频的合成方法，绕过了对中间3D模型或面部标志的需求。我们的方法确保了视频中无缝的帧过渡和一致的身份保持，产生了高度表现力和逼真的动画。实验结果表明，EMO不仅能够生成令人信服的说话视频，还能以各种风格生成唱歌视频，在表现力和真实感方面显著超越现有的最先进方法。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"多模态\",\n  \"标签\": [\"talking head video generation\", \"audio-to-video synthesis\", \"expressiveness\", \"realism\", \"EMO\"]\n}\n```",
  "tag_info": {
    "主要领域": "多模态",
    "标签": [
      "talking head video generation",
      "audio-to-video synthesis",
      "expressiveness",
      "realism",
      "EMO"
    ]
  }
}
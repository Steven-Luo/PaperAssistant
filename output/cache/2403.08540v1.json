{
  "id": "2403.08540v1",
  "title": "Language models scale reliably with over-training and on downstream tasks",
  "pdf_url": "http://arxiv.org/pdf/2403.08540v1",
  "raw_tldr": "动机\t缩放法则是发展语言模型的有用指南，但当前的缩放研究与语言模型最终的训练和评估之间仍存在差距。例如，缩放通常在计算最优训练体制下研究，但实际上，模型经常被过度训练以降低推理成本。此外，缩放法则大多预测下一个令牌预测的损失，但最终模型是基于下游任务性能进行比较的。\n方法\t本文通过创建一个由0.011B到6.9B参数的104个模型组成的测试平台，这些模型在三种数据分布上训练了不同数量的令牌，来解决这两个缺陷。首先，研究了过度训练体制下的缩放。拟合了能够在模型参数数量和训练令牌与参数比例两个维度上外推的缩放法则。其次，通过幂律将语言模型的困惑度与其下游任务性能相关联。\n结果\t这使我们能够预测1.4B参数、900B令牌运行（即32倍过度训练）和6.9B参数、138B令牌运行的验证损失，每次实验的计算量减少了300倍。我们还使用这一法则预测了上述两个模型在下游任务中的平均top-1错误率，这些实验的计算量减少了20倍。",
  "tldr": {
    "动机": "缩放法则是发展语言模型的有用指南，但当前的缩放研究与语言模型最终的训练和评估之间仍存在差距。例如，缩放通常在计算最优训练体制下研究，但实际上，模型经常被过度训练以降低推理成本。此外，缩放法则大多预测下一个令牌预测的损失，但最终模型是基于下游任务性能进行比较的。",
    "方法": "本文通过创建一个由0.011B到6.9B参数的104个模型组成的测试平台，这些模型在三种数据分布上训练了不同数量的令牌，来解决这两个缺陷。首先，研究了过度训练体制下的缩放。拟合了能够在模型参数数量和训练令牌与参数比例两个维度上外推的缩放法则。其次，通过幂律将语言模型的困惑度与其下游任务性能相关联。",
    "结果": "这使我们能够预测1.4B参数、900B令牌运行（即32倍过度训练）和6.9B参数、138B令牌运行的验证损失，每次实验的计算量减少了300倍。我们还使用这一法则预测了上述两个模型在下游任务中的平均top-1错误率，这些实验的计算量减少了20倍。"
  },
  "summary_cn": "缩放定律是开发语言模型的有用指南，但当前的缩放研究与最终训练和评估语言模型之间仍存在差距。例如，缩放通常在计算最优训练制度下研究（即“Chinchilla最优”制度）；然而，在实践中，模型经常被过度训练以降低推理成本。此外，缩放定律大多预测下一个令牌预测的损失，但最终模型是基于下游任务性能进行比较的。在本文中，我们解决了这两个缺点。为此，我们创建了一个由104个模型组成的测试平台，这些模型的参数从0.011B到6.9B不等，使用不同数量的令牌在三个数据分布上进行训练。首先，我们研究了过度训练制度下的缩放。我们拟合了缩放定律，以便在模型参数数量和训练令牌与参数的比率两个方面进行外推。这使我们能够预测1.4B参数、900B令牌运行（即32倍过度训练）和6.9B参数、138B令牌运行的验证损失——每个实验的计算量减少了300倍。其次，我们通过幂律将语言模型的困惑度与其下游任务性能联系起来。我们使用这一定律来预测上述两个模型在下游任务中的平均top-1错误，这些实验的计算量减少了20倍。我们的实验可在https://github.com/mlfoundations/scaling 上找到。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"scaling laws\",\n    \"language models\",\n    \"compute-optimal training regime\",\n    \"over-trained\",\n    \"downstream task performance\",\n    \"model parameters\",\n    \"training tokens\",\n    \"validation loss\",\n    \"perplexity\",\n    \"top-1 error\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "scaling laws",
      "language models",
      "compute-optimal training regime",
      "over-trained",
      "downstream task performance",
      "model parameters",
      "training tokens",
      "validation loss",
      "perplexity",
      "top-1 error"
    ]
  }
}
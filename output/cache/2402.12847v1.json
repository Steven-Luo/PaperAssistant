{
  "id": "2402.12847v1",
  "title": "Instruction-tuned Language Models are Better Knowledge Learners",
  "pdf_url": "http://arxiv.org/pdf/2402.12847v1",
  "raw_tldr": "动机\t为了使基于大型语言模型(LLM)的助手能够有效适应不断变化的信息需求，必须能够通过在新数据上继续训练来更新它们的事实知识。然而，标准的做法（在新文档上继续预训练，随后进行问题-答案(QA)对的指令调优）存在问题，即LLM在此方法下难以回答问题，尽管文档的困惑度最小化了。\n方法\t提出了预指令调优(PIT)方法，该方法在文档训练之前对问题进行指令调优。这与标准的指令调优（在文档训练之后学习如何提取知识）形成对比。\n结果\tPIT显著增强了LLM从新文档中吸收知识的能力，比标准指令调优提高了17.8%。",
  "tldr": {
    "动机": "为了使基于大型语言模型(LLM)的助手能够有效适应不断变化的信息需求，必须能够通过在新数据上继续训练来更新它们的事实知识。然而，标准的做法（在新文档上继续预训练，随后进行问题-答案(QA)对的指令调优）存在问题，即LLM在此方法下难以回答问题，尽管文档的困惑度最小化了。",
    "方法": "提出了预指令调优(PIT)方法，该方法在文档训练之前对问题进行指令调优。这与标准的指令调优（在文档训练之后学习如何提取知识）形成对比。",
    "结果": "PIT显著增强了LLM从新文档中吸收知识的能力，比标准指令调优提高了17.8%。"
  },
  "summary_cn": "为了使基于大型语言模型（LLM）的助手能够有效地适应不断变化的信息需求，必须能够通过持续在新数据上训练来更新它们的事实知识。常规的做法包括在新文档上继续预训练，然后在问题-答案（QA）对上进行指令调优。然而，我们发现，尽管文档的困惑度被最小化，但采用这种方法训练的LLM在回答问题时仍然存在困难。我们发现，QA对通常是直接的，而文档则更为复杂，以一种错综复杂的方式编织了许多事实陈述。因此，我们假设，在文档的持续预训练之前，先让LLM接触QA对是有益的，这样从复杂文档中编码知识的过程就会考虑到通过问题访问这些知识的方式。基于此，我们提出了预指令调优（PIT）方法，该方法在文档训练之前先对问题进行指令调优。这与标准的指令调优形成对比，后者是在文档训练之后学习如何提取知识。大量实验和消融研究表明，PIT显著提高了LLM从新文档中吸收知识的能力，其性能超过标准指令调优17.8%。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"large language model\",\n    \"continued training\",\n    \"question-answer pairs\",\n    \"pre-instruction-tuning\",\n    \"knowledge extraction\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "large language model",
      "continued training",
      "question-answer pairs",
      "pre-instruction-tuning",
      "knowledge extraction"
    ]
  }
}
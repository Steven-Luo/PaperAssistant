{
  "id": "2403.01444v1",
  "title": "3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos",
  "pdf_url": "http://arxiv.org/pdf/2403.01444v1",
  "raw_tldr": "动机\t构建动态场景的逼真自由视角视频(FVVs)仍然是一个挑战性的任务。尽管当前的神经渲染技术取得了显著进步，但这些方法通常需要完整的视频序列进行离线训练，并且不能实现实时渲染。\n方法\t我们介绍了3DGStream，一种为实时动态场景的高效FVV流设计的方法。该方法利用3D高斯(3DGs)来表示场景，并采用紧凑的神经变换缓存(NTC)来模拟3DGs的平移和旋转，显著减少了每个FVV帧所需的训练时间和存储空间。此外，我们提出了一种适应性3DG添加策略来处理动态场景中出现的新对象。\n结果\t3DGStream在渲染速度、图像质量、训练时间和模型存储方面与最先进的方法相比，实现了竞争性的性能，实现了每帧12秒内的快速即时重建和200 FPS的实时渲染。",
  "tldr": {
    "动机": "构建动态场景的逼真自由视角视频(FVVs)仍然是一个挑战性的任务。尽管当前的神经渲染技术取得了显著进步，但这些方法通常需要完整的视频序列进行离线训练，并且不能实现实时渲染。",
    "方法": "我们介绍了3DGStream，一种为实时动态场景的高效FVV流设计的方法。该方法利用3D高斯(3DGs)来表示场景，并采用紧凑的神经变换缓存(NTC)来模拟3DGs的平移和旋转，显著减少了每个FVV帧所需的训练时间和存储空间。此外，我们提出了一种适应性3DG添加策略来处理动态场景中出现的新对象。",
    "结果": "3DGStream在渲染速度、图像质量、训练时间和模型存储方面与最先进的方法相比，实现了竞争性的性能，实现了每帧12秒内的快速即时重建和200 FPS的实时渲染。"
  },
  "summary_cn": "从多视角视频中构建动态场景的逼真自由视点视频（FVVs）仍然是一项挑战性的工作。尽管当前的神经渲染技术取得了显著的进步，但这些方法通常需要完整的视频序列进行离线训练，并且不能实现实时渲染。为了解决这些限制，我们引入了3DGStream，这是一种为实时动态场景的高效FVV流设计的方法。我们的方法在12秒内实现了快速的即时逐帧重建，并且能以200 FPS的速度实现实时渲染。具体来说，我们利用3D高斯（3DGs）来表示场景。我们没有采用直接优化每帧的3DGs的简单方法，而是采用了一个紧凑的神经转换缓存（NTC）来模拟3DGs的平移和旋转，显著减少了每个FVV帧所需的训练时间和存储空间。此外，我们提出了一种适应性3DG添加策略，以处理动态场景中出现的新对象。实验表明，与最先进的方法相比，3DGStream在渲染速度、图像质量、训练时间和模型存储方面都达到了有竞争力的性能。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"CV\",\n  \"标签\": [\n    \"Free-Viewpoint Videos\",\n    \"Neural Rendering\",\n    \"Real-Time Rendering\",\n    \"3D Gaussians\",\n    \"Dynamic Scenes\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "CV",
    "标签": [
      "Free-Viewpoint Videos",
      "Neural Rendering",
      "Real-Time Rendering",
      "3D Gaussians",
      "Dynamic Scenes"
    ]
  }
}
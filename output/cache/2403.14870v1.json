{
  "id": "2403.14870v1",
  "title": "VidLA: Video-Language Alignment at Scale",
  "pdf_url": "http://arxiv.org/pdf/2403.14870v1",
  "raw_tldr": "动机\t目前的视频-语言对齐方法存在两大限制：一是它们不能捕捉短程和长程的时间依赖性，通常采用复杂的分层深度网络架构，难以与现有的预训练图像-文本基础模型集成；二是现有的视频-语言对齐工作由于缺乏语义对齐的大规模训练数据而遇到困难。\n方法\t本文提出了一种名为VidLA的视频-语言对齐方法，通过使用一组在不同时间分辨率上以分层方式操作的数据令牌，以及采用简单的双塔架构，使模型能够与预训练的图像-文本基础模型初始化，从而提高性能。此外，通过利用最新的大型语言模型（LLMs）来策划迄今为止最大的、具有更好视觉基础的视频-语言数据集。\n结果\t实证结果显示，所提出的方法在多个检索基准上超越了最先进的方法，特别是在较长视频上表现出色，并在分类基准上也具有竞争力。",
  "tldr": {
    "动机": "目前的视频-语言对齐方法存在两大限制：一是它们不能捕捉短程和长程的时间依赖性，通常采用复杂的分层深度网络架构，难以与现有的预训练图像-文本基础模型集成；二是现有的视频-语言对齐工作由于缺乏语义对齐的大规模训练数据而遇到困难。",
    "方法": "本文提出了一种名为VidLA的视频-语言对齐方法，通过使用一组在不同时间分辨率上以分层方式操作的数据令牌，以及采用简单的双塔架构，使模型能够与预训练的图像-文本基础模型初始化，从而提高性能。此外，通过利用最新的大型语言模型（LLMs）来策划迄今为止最大的、具有更好视觉基础的视频-语言数据集。",
    "结果": "实证结果显示，所提出的方法在多个检索基准上超越了最先进的方法，特别是在较长视频上表现出色，并在分类基准上也具有竞争力。"
  },
  "summary_cn": "在本文中，我们提出了VidLA，这是一种大规模视频-语言对齐方法。之前的视频-语言对齐方法有两个主要限制。首先，它们无法捕捉短期和长期的时间依赖性，并且通常采用复杂的层次化深度网络架构，这些架构难以与现有的预训练图像-文本基础模型集成。为了有效解决这个限制，我们保持网络架构的简单，并使用一组在层次上以不同时间分辨率操作的数据令牌，以解释视频的时间层次性。通过采用简单的双塔架构，我们能够用预训练的图像-文本基础模型初始化我们的视频-语言模型，从而提升最终性能。其次，现有的视频-语言对齐工作由于缺乏语义对齐的大规模训练数据而受挫。为了克服这一点，我们利用最近的大型语言模型（LLMs）来策划迄今为止最大的视频-语言数据集，以实现更好的视觉定位。此外，与仅包含短片段的现有视频-文本数据集不同，我们的数据集包含了不同持续时间的视频片段，以帮助我们的时间层次性数据令牌在不同时间尺度上提取更好的表示。总的来说，实证结果表明，我们提出的方法在多个检索基准测试上超越了最先进的方法，尤其是在较长视频上，而且在分类基准测试上表现出竞争力。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"多模态\",\n  \"标签\": [\n    \"视频语言对齐\",\n    \"时态依赖\",\n    \"数据标记\",\n    \"大规模训练数据\",\n    \"视觉定位\",\n    \"检索基准\",\n    \"分类基准\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "多模态",
    "标签": [
      "视频语言对齐",
      "时态依赖",
      "数据标记",
      "大规模训练数据",
      "视觉定位",
      "检索基准",
      "分类基准"
    ]
  }
}
{
  "id": "2403.09394v1",
  "title": "GiT: Towards Generalist Vision Transformer through Universal Language Interface",
  "pdf_url": "http://arxiv.org/pdf/2403.09394v1",
  "raw_tldr": "动机\t本文旨在通过设计一个简单而有效的框架GiT，利用通用的ViT模型，将多层Transformer架构（如GPT）的通用性从大型语言模型（LLM）扩展到视觉领域，解决视觉任务中特定模块需求（如检测的边界框头和分割的像素解码器）对Transformer应用的限制。\n方法\t通过设计一个通用语言接口，使得自回归解码能够灵活地统一不同的视觉任务（如图像级理解、稀疏感知到密集预测），整个模型仅由ViT组成，无需任何特定添加，实现了架构的显著简化，并在五个代表性基准上进行联合训练，无需任务特定的微调。\n结果\tGiT建立了通用性能的新基准，并促进了任务间的相互增强，与孤立训练相比取得了显著改进。通过使用27个数据集进一步丰富训练，GiT在多个任务上实现了强大的零样本结果，展示了缩小视觉与语言架构差距的潜力。",
  "tldr": {
    "动机": "本文旨在通过设计一个简单而有效的框架GiT，利用通用的ViT模型，将多层Transformer架构（如GPT）的通用性从大型语言模型（LLM）扩展到视觉领域，解决视觉任务中特定模块需求（如检测的边界框头和分割的像素解码器）对Transformer应用的限制。",
    "方法": "通过设计一个通用语言接口，使得自回归解码能够灵活地统一不同的视觉任务（如图像级理解、稀疏感知到密集预测），整个模型仅由ViT组成，无需任何特定添加，实现了架构的显著简化，并在五个代表性基准上进行联合训练，无需任务特定的微调。",
    "结果": "GiT建立了通用性能的新基准，并促进了任务间的相互增强，与孤立训练相比取得了显著改进。通过使用27个数据集进一步丰富训练，GiT在多个任务上实现了强大的零样本结果，展示了缩小视觉与语言架构差距的潜力。"
  },
  "summary_cn": "本文提出了一个简单而有效的框架，称为GiT，它可以同时适用于各种视觉任务，仅需使用标准的ViT（Vision Transformer）。受到在大型语言模型（LLMs）中广泛使用的多层Transformer架构（例如，GPT）的普遍性的启发，我们寻求将其范围扩大，使其成为一个强大的视觉基础模型（VFM）。然而，与语言建模不同，视觉任务通常需要特定的模块，例如检测的边界框头和分割的像素解码器，这极大地阻碍了在视觉领域应用强大的多层transformers。为了解决这个问题，我们设计了一个通用的语言接口，使得成功的自回归解码能够熟练地统一各种视觉任务，从图像级理解（例如，标题生成），到稀疏感知（例如，检测），再到密集预测（例如，分割）。基于上述设计，整个模型仅由一个ViT组成，没有任何特定的添加，提供了显著的架构简化。GiT是一个多任务视觉模型，跨五个代表性基准联合训练，无需任务特定的微调。有趣的是，我们的GiT在通才性能上建立了一个新的基准，并促进了任务间的相互增强，与孤立训练相比，带来了显著的改进。这反映了在LLMs中观察到的类似影响。通过使用27个数据集进一步丰富训练，GiT在各种任务上实现了强大的零样本结果。由于其设计简单，这一范式有望缩小视觉与语言之间的架构差距。代码和模型将在\\url{https://github.com/Haiyang-W/GiT}上提供。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"CV\",\n  \"标签\": [\n    \"ViT\",\n    \"vision tasks\",\n    \"universal language interface\",\n    \"multi-task\",\n    \"zero-shot\",\n    \"architectural simplification\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "CV",
    "标签": [
      "ViT",
      "vision tasks",
      "universal language interface",
      "multi-task",
      "zero-shot",
      "architectural simplification"
    ]
  }
}
{
  "id": "2401.10020v2",
  "title": "Self-Rewarding Language Models",
  "pdf_url": "http://arxiv.org/pdf/2401.10020v2",
  "raw_tldr": "动机\t我们认为，为了实现超人类智能体，未来的模型需要超人类的反馈来提供足够的训练信号。当前的方法通常从人类偏好中训练奖励模型，这可能会受到人类性能水平的限制，其次，这些独立的冻结奖励模型在LLM训练期间无法学习改进。\n方法\t在这项工作中，我们研究了自奖励语言模型，其中语言模型本身通过LLM-as-a-Judge提示在训练期间提供自己的奖励。我们展示了在迭代DPO训练期间，不仅指令遵循能力得到了提高，而且还能够向自己提供高质量的奖励。\n结果\t在我们的方法的三次迭代上对Llama 2 70B进行微调，产生了一个在AlpacaEval 2.0排行榜上超越了许多现有系统的模型，包括Claude 2、Gemini Pro和GPT-4 0613。尽管还有很多需要探索，但这项工作为模型能够在两个轴向上持续改进打开了大门。",
  "tldr": {
    "动机": "我们认为，为了实现超人类智能体，未来的模型需要超人类的反馈来提供足够的训练信号。当前的方法通常从人类偏好中训练奖励模型，这可能会受到人类性能水平的限制，其次，这些独立的冻结奖励模型在LLM训练期间无法学习改进。",
    "方法": "在这项工作中，我们研究了自奖励语言模型，其中语言模型本身通过LLM-as-a-Judge提示在训练期间提供自己的奖励。我们展示了在迭代DPO训练期间，不仅指令遵循能力得到了提高，而且还能够向自己提供高质量的奖励。",
    "结果": "在我们的方法的三次迭代上对Llama 2 70B进行微调，产生了一个在AlpacaEval 2.0排行榜上超越了许多现有系统的模型，包括Claude 2、Gemini Pro和GPT-4 0613。尽管还有很多需要探索，但这项工作为模型能够在两个轴向上持续改进打开了大门。"
  },
  "summary_cn": "我们认为，要实现超人类智能体，未来的模型需要超人类的反馈，以提供足够的训练信号。目前的方法通常是从人类偏好中训练奖励模型，这可能会受到人类性能水平的限制；其次，这些独立的固定奖励模型在大型语言模型（LLM）训练期间无法学习改进。在这项工作中，我们研究了自奖励语言模型，其中利用LLM作为评判的提示，让语言模型本身在训练过程中提供自我奖励。我们展示了在迭代DPO训练过程中，不仅指令遵循能力得到了提升，而且还提高了向自身提供高质量奖励的能力。在我们的方法上进行三次迭代的微调Llama 2 70B模型，在AlpacaEval 2.0排行榜上超越了许多现有系统，包括Claude 2、Gemini Pro和GPT-4 0613。虽然还有很多需要探索的地方，但这项工作为模型能够在两个方面持续改进打开了大门。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"Machine Learning\",\n  \"标签\": [\n    \"Self-Rewarding Language Models\",\n    \"LLM-as-a-Judge\",\n    \"Iterative DPO training\",\n    \"Fine-tuning\",\n    \"Superhuman agents\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "Machine Learning",
    "标签": [
      "Self-Rewarding Language Models",
      "LLM-as-a-Judge",
      "Iterative DPO training",
      "Fine-tuning",
      "Superhuman agents"
    ]
  }
}
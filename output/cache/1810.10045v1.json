{
  "id": "1810.10045v1",
  "raw_tldr": "动机\t使用Zipf定律扩展语言模型（LM），以利用更多的训练数据和GPU，因为在自然语言处理应用中，如语音识别和机器翻译，LM扮演着关键角色，而且普遍认为更多的数据会带来更好的结果。\n方法\t通过利用Zipf定律对常见词汇和字符序列的参数进行分组来解决内存（GPU内部）和通信（GPU之间）的瓶颈问题，从而降低了每个GPU的内存和通信的渐进复杂度，并在四个公开的大型数据集上进行了实证研究。\n结果\t当GPU数量增加到64时，训练时间加速比达到6.7倍（字符LM）和6.3倍（词LM），准确率损失可以忽略不计。在192个GPU上对Tieba数据集进行的弱扩展实验显示，通过训练93GB的数据（比公开的最先进数据集大2.5倍），LM预测准确率提高了35%，而训练时间仅增加了1.25倍。",
  "tldr": {
    "动机": "使用Zipf定律扩展语言模型（LM），以利用更多的训练数据和GPU，因为在自然语言处理应用中，如语音识别和机器翻译，LM扮演着关键角色，而且普遍认为更多的数据会带来更好的结果。",
    "方法": "通过利用Zipf定律对常见词汇和字符序列的参数进行分组来解决内存（GPU内部）和通信（GPU之间）的瓶颈问题，从而降低了每个GPU的内存和通信的渐进复杂度，并在四个公开的大型数据集上进行了实证研究。",
    "结果": "当GPU数量增加到64时，训练时间加速比达到6.7倍（字符LM）和6.3倍（词LM），准确率损失可以忽略不计。在192个GPU上对Tieba数据集进行的弱扩展实验显示，通过训练93GB的数据（比公开的最先进数据集大2.5倍），LM预测准确率提高了35%，而训练时间仅增加了1.25倍。"
  },
  "summary_cn": "我们展示了如何利用齐普夫定律（Zipf's Law）扩大语言模型（LM）的规模，以利用更多的训练数据和更多的GPU资源。语言模型在许多重要的自然语言应用中扮演关键角色，例如语音识别和机器翻译。扩大语言模型的规模非常重要，因为社区普遍接受“没有什么数据比更多的数据更好”的观点。最终，我们希望能够在数TB（万亿字）的文本上进行训练。由于各种瓶颈，尤其是内存（GPU内部）和通信（GPU之间）的瓶颈，现代训练方法距离这一目标还很远。本文展示了如何利用齐普夫定律通过对常见词汇和字符序列的参数进行分组来解决这些瓶颈，因为$U \\ll N$，其中$U$是唯一词汇（类型）的数量，$N$是训练集的大小（标记）。对于局部批量大小$K$，$G$个GPU和一个$D$维嵌入矩阵，我们将原始的每GPU内存和通信的渐进复杂度从$\\Theta(GKD)$降低到$\\Theta(GK + UD)$。经验上，我们发现在四个公开可用的大型数据集上$U \\propto (GK)^{0.64}$。当我们将GPU数量扩大到64，即增加8倍时，训练时间的加速比高达6.7$\\times$（对于字符LM）和6.3$\\times$（对于词LM），准确率损失可以忽略不计。我们在192个GPU上对Tieba数据集进行的弱扩展显示，通过在93GB的数据上训练（比公开可用的SOTA数据集大2.5倍），LM预测准确率提高了35%，但训练时间仅增加了1.25倍，相比之下，同一数据集的3GB在6个GPU上运行。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"语言模型\",\n    \"Zipf's Law\",\n    \"大规模训练\",\n    \"GPU优化\",\n    \"自然语言处理应用\",\n    \"数据扩展\",\n    \"训练效率\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "语言模型",
      "Zipf's Law",
      "大规模训练",
      "GPU优化",
      "自然语言处理应用",
      "数据扩展",
      "训练效率"
    ]
  }
}
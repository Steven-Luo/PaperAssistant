{
  "id": "2403.18814v1",
  "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models",
  "pdf_url": "http://arxiv.org/pdf/2403.18814v1",
  "raw_tldr": "动机：为了缩小视觉语言模型（VLMs）与先进模型（如GPT-4和Gemini）之间的性能差距，提高VLMs在视觉对话和推理方面的表现。\n方法：通过三个方面增强VLMs的潜力：使用额外的视觉编码器提高高分辨率视觉标记的质量而不增加数量，构建高质量数据集以促进精确的图像理解和基于推理的生成，以及VLM引导的生成。\n结果：Mini-Gemini在几个零样本基准测试中取得了领先性能，甚至超过了开发的私有模型，并且支持从2B到34B的一系列密集和MoE大型语言模型（LLMs）。",
  "tldr": {
    "动机：为了缩小视觉语言模型（VLMs）与先进模型（如GPT-4和Gemini）之间的性能差距，提高VLMs在视觉对话和推理方面的表现。": "",
    "方法：通过三个方面增强VLMs的潜力：使用额外的视觉编码器提高高分辨率视觉标记的质量而不增加数量，构建高质量数据集以促进精确的图像理解和基于推理的生成，以及VLM引导的生成。": "",
    "结果：Mini-Gemini在几个零样本基准测试中取得了领先性能，甚至超过了开发的私有模型，并且支持从2B到34B的一系列密集和MoE大型语言模型（LLMs）。": "",
    "动机": "",
    "方法": "",
    "结果": ""
  },
  "summary_cn": "在这项工作中，我们介绍了Mini-Gemini，这是一个简单而有效的框架，用于增强多模态视觉语言模型（VLMs）。尽管VLMs在促进基本视觉对话和推理方面取得了进步，但与GPT-4和Gemini等先进模型相比，性能差距仍然存在。我们试图通过挖掘VLMs的潜力，从三个方面来提高性能和实现任何到任何的工作流程，即高分辨率视觉标记、高质量数据和VLM引导的生成。为了增强视觉标记，我们提议利用额外的视觉编码器进行高分辨率细化，而不增加视觉标记的数量。我们进一步构建了一个高质量的数据集，以促进精确的图像理解和基于推理的生成，扩大了当前VLMs的操作范围。总的来说，Mini-Gemini进一步挖掘了VLMs的潜力，并赋予了当前框架同时进行图像理解、推理和生成的能力。Mini-Gemini支持一系列密集和MoE大型语言模型（LLMs），从2B到34B。它在几个零样本基准测试中显示出领先的性能，甚至超过了开发的私有模型。代码和模型可在 https://github.com/dvlab-research/MiniGemini 上获取。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"多模态\",\n  \"标签\": [\n    \"Vision Language Models (VLMs)\",\n    \"多模态学习\",\n    \"图像理解\",\n    \"自然语言处理\",\n    \"高质量数据集\",\n    \"视觉编码器\",\n    \"推理\",\n    \"生成模型\",\n    \"大型语言模型 (LLMs)\",\n    \"零样本基准测试\",\n    \"代码开源\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "多模态",
    "标签": [
      "Vision Language Models (VLMs)",
      "多模态学习",
      "图像理解",
      "自然语言处理",
      "高质量数据集",
      "视觉编码器",
      "推理",
      "生成模型",
      "大型语言模型 (LLMs)",
      "零样本基准测试",
      "代码开源"
    ]
  }
}
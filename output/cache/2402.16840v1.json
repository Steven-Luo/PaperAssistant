{
  "id": "2402.16840v1",
  "raw_tldr": "动机\t\"Bigger the better\"的趋势并不适合需要在设备上处理、能源效率高、内存占用低和响应效率高的场景，这些场景对隐私、安全和可持续部署至关重要。\n方法\t通过引入一个精确且完全透明的开源0.5亿参数的小型语言模型（SLM）MobiLlama，该模型从一个较大的模型开始，应用精心的参数共享方案来减少预训练和部署成本，专门满足资源受限计算的特定需求，并强调在减少资源需求的同时提升性能。\n结果\tMobiLlama作为一个SLM设计，不仅弥补了开源SLM的空白，还确保了完全的透明度，提供了完整的训练数据管道、训练代码、模型权重和超过300个检查点以及评估代码。",
  "tldr": {
    "动机": "\"Bigger the better\"的趋势并不适合需要在设备上处理、能源效率高、内存占用低和响应效率高的场景，这些场景对隐私、安全和可持续部署至关重要。",
    "方法": "通过引入一个精确且完全透明的开源0.5亿参数的小型语言模型（SLM）MobiLlama，该模型从一个较大的模型开始，应用精心的参数共享方案来减少预训练和部署成本，专门满足资源受限计算的特定需求，并强调在减少资源需求的同时提升性能。",
    "结果": "MobiLlama作为一个SLM设计，不仅弥补了开源SLM的空白，还确保了完全的透明度，提供了完整的训练数据管道、训练代码、模型权重和超过300个检查点以及评估代码。"
  },
  "summary_cn": "\"越大越好\"一直是近期大型语言模型（LLMs）发展的主导趋势。然而，LLMs并不适合需要在设备上处理、能源效率高、内存占用低和响应效率高的场景。这些要求对于隐私、安全和可持续部署至关重要。本文通过探索设计精准且高效的小型语言模型（SLMs）来应对资源受限设备的挑战，探讨了“少即是多”的范式。我们的主要贡献是引入了一个精确且完全透明的开源0.5亿（0.5B）参数SLM，名为MobiLlama，专门满足资源受限计算的特定需求，并强调在减少资源需求的同时提升性能。MobiLlama是一种SLM设计，它从一个较大的模型开始，并应用一种谨慎的参数共享方案来降低预训练和部署成本。我们的工作不仅努力弥补开源SLMs的差距，还确保了完全的透明度，其中包括完整的训练数据管道、训练代码、模型权重和超过300个检查点以及评估代码，可在以下网址获取：https://github.com/mbzuai-oryx/MobiLlama。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"Large Language Models\",\n    \"Small Language Models\",\n    \"Resource-Constrained Devices\",\n    \"Parameter Sharing\",\n    \"Open-Source\",\n    \"Energy Efficiency\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "Large Language Models",
      "Small Language Models",
      "Resource-Constrained Devices",
      "Parameter Sharing",
      "Open-Source",
      "Energy Efficiency"
    ]
  }
}
{
  "summary_cn": "近期的研究表明，基于注意力的语言模型在回忆能力上表现出色，即能够将生成内容与之前在上下文中见过的标记联系起来。然而，在推理过程中，基于注意力的模型的效率受到KV缓存过度占用内存的限制。在这项工作中，我们探讨了是否可以在不牺牲回忆能力的情况下提高语言模型的效率（例如，通过减少内存消耗）。通过对一系列架构应用实验和理论，我们发现了模型状态大小和回忆能力之间的一个关键权衡。我们展示了高效的注意力替代方案（例如，H3、Mamba、RWKV）维持一个固定大小的循环状态，但在回忆方面表现不佳。我们提出了一个简单的架构BASED，它结合了线性注意力和滑动窗口注意力。通过改变BASED窗口大小和线性注意力特征维度，我们可以调整状态大小，并在回忆-内存权衡曲线的帕累托前沿进行遍历，一端恢复注意力的全部质量，另一端则达到注意力替代方案的小状态大小。我们训练了多达1.3亿参数的语言模型，并展示了BASED在复杂度方面与最强的次二次方模型（例如，Mamba）相匹配，并在真实世界的回忆密集任务上以6.22的准确度得分超越了它们。线性注意力的实现通常不如优化的标准注意力实现高效。为了使BASED具有竞争力，我们开发了IO感知算法，使得在使用1.3亿参数模型生成1024个标记时，语言生成的吞吐量比FlashAttention-2高出24倍。这项工作的代码提供在：https://github.com/HazyResearch/based。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"attention-based language models\",\n    \"memory efficiency\",\n    \"recall\",\n    \"linear attention\",\n    \"sliding window attention\",\n    \"language model efficiency\",\n    \"tradeoff analysis\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "attention-based language models",
      "memory efficiency",
      "recall",
      "linear attention",
      "sliding window attention",
      "language model efficiency",
      "tradeoff analysis"
    ]
  },
  "id": "2402.18668v1",
  "raw_tldr": "动机\t最近的研究表明，基于注意力的语言模型在回忆能力上表现出色，即能够将生成内容与之前在上下文中看到的标记联系起来。然而，基于注意力的模型在推理过程中由于KV缓存的大量内存消耗而效率受限。本工作探讨了是否可以在不牺牲回忆能力的情况下提高语言模型的效率（例如通过减少内存消耗）。\n方法\t通过对广泛的架构应用实验和理论，我们发现模型状态大小和回忆能力之间存在关键的权衡。我们展示了高效的注意力替代方案（例如H3、Mamba、RWKV）维持固定大小的循环状态，但在回忆方面存在困难。我们提出了一个简单的架构BASED，结合了线性注意力和滑动窗口注意力。通过改变BASED窗口大小和线性注意力特征维度，我们可以调整状态大小并遍历回忆-内存权衡曲线的帕累托前沿，一端恢复注意力的全部质量，另一端则恢复注意力替代方案的小状态大小。\n结果\tBASED训练了多达1.3b参数的语言模型，并显示在困惑度上与最强的次二次方模型（例如Mamba）相匹配，在真实世界的回忆密集任务上比它们高出6.22个准确度点。为了使BASED具有竞争力，我们开发了IO感知算法，使得在使用1.3b参数模型生成1024个标记时，语言生成的吞吐量比FlashAttention-2高出24倍。",
  "tldr": {
    "动机": "最近的研究表明，基于注意力的语言模型在回忆能力上表现出色，即能够将生成内容与之前在上下文中看到的标记联系起来。然而，基于注意力的模型在推理过程中由于KV缓存的大量内存消耗而效率受限。本工作探讨了是否可以在不牺牲回忆能力的情况下提高语言模型的效率（例如通过减少内存消耗）。",
    "方法": "通过对广泛的架构应用实验和理论，我们发现模型状态大小和回忆能力之间存在关键的权衡。我们展示了高效的注意力替代方案（例如H3、Mamba、RWKV）维持固定大小的循环状态，但在回忆方面存在困难。我们提出了一个简单的架构BASED，结合了线性注意力和滑动窗口注意力。通过改变BASED窗口大小和线性注意力特征维度，我们可以调整状态大小并遍历回忆-内存权衡曲线的帕累托前沿，一端恢复注意力的全部质量，另一端则恢复注意力替代方案的小状态大小。",
    "结果": "BASED训练了多达1.3b参数的语言模型，并显示在困惑度上与最强的次二次方模型（例如Mamba）相匹配，在真实世界的回忆密集任务上比它们高出6.22个准确度点。为了使BASED具有竞争力，我们开发了IO感知算法，使得在使用1.3b参数模型生成1024个标记时，语言生成的吞吐量比FlashAttention-2高出24倍。"
  }
}
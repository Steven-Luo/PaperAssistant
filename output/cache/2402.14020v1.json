{
  "id": "2402.14020v1",
  "title": "Coercing LLMs to do and reveal (almost) anything",
  "pdf_url": "http://arxiv.org/pdf/2402.14020v1",
  "raw_tldr": "动机\t大型语言模型（LLMs）最近被发现可以通过对抗性攻击被“越狱”以发表有害声明，本工作认为对LLMs的对抗性攻击的范围远不止于此。\n方法\t通过一系列具体示例，讨论、分类和系统化了强制模型产生各种非预期行为（如误导、模型控制、拒绝服务或数据提取）的攻击，并在受控实验中分析这些攻击。\n结果\t发现许多攻击源于LLMs具有编码能力的预训练实践，以及常见LLM词汇表中仍存在的、出于安全原因应当移除的奇怪“故障”令牌。",
  "tldr": {
    "动机": "大型语言模型（LLMs）最近被发现可以通过对抗性攻击被“越狱”以发表有害声明，本工作认为对LLMs的对抗性攻击的范围远不止于此。",
    "方法": "通过一系列具体示例，讨论、分类和系统化了强制模型产生各种非预期行为（如误导、模型控制、拒绝服务或数据提取）的攻击，并在受控实验中分析这些攻击。",
    "结果": "发现许多攻击源于LLMs具有编码能力的预训练实践，以及常见LLM词汇表中仍存在的、出于安全原因应当移除的奇怪“故障”令牌。"
  },
  "summary_cn": "最近的研究表明，对大型语言模型（LLMs）的对抗性攻击可以使模型产生有害声明，这种现象被称为“越狱”。在这项工作中，我们认为对LLMs的对抗性攻击的范围远不止于越狱。我们提供了可能的攻击面和攻击目标的广泛概述。基于一系列具体示例，我们讨论、分类并系统化了强制模型产生各种非预期行为的攻击，如误导、模型控制、拒绝服务或数据提取。我们在控制实验中分析了这些攻击，并发现它们中的许多源于预训练具有编码能力的LLMs的做法，以及常见LLM词汇中仍然存在的、出于安全原因应该被移除的奇怪“故障”令牌。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\"adversarial attacks\", \"large language models\", \"security\", \"pre-training\", \"glitch tokens\"]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "adversarial attacks",
      "large language models",
      "security",
      "pre-training",
      "glitch tokens"
    ]
  }
}
{
  "id": "2403.15447v1",
  "title": "Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression",
  "pdf_url": "http://arxiv.org/pdf/2403.15447v1",
  "raw_tldr": "动机：本研究旨在深入评估压缩大型语言模型（LLMs）的安全性和可信度，因为现有的压缩方法虽然在保持良性任务性能方面取得了显著进展，但对于压缩可能带来的安全和可信度风险却鲜有关注。\n\n方法：研究使用了五种最新的压缩技术对三种领先的大型语言模型进行了评估，覆盖了八个可信度维度，通过实验揭示了压缩与可信度之间复杂的相互作用。\n\n结果：量化是目前在同时实现效率和可信度方面比剪枝更有效的方法。例如，4位量化的模型保持了原始模型的可信度，而模型剪枝即使在50%的稀疏度下也会显著降低可信度。适度位宽的量化可能意外地改善某些可信度维度，如伦理和公平性。相反，极端量化到非常低的位水平（3位）往往会显著降低可信度。这些增加的风险仅通过观察良性性能是无法发现的，从而在实践中强制要求进行全面的可信度评估。",
  "tldr": {
    "动机：本研究旨在深入评估压缩大型语言模型（LLMs）的安全性和可信度，因为现有的压缩方法虽然在保持良性任务性能方面取得了显著进展，但对于压缩可能带来的安全和可信度风险却鲜有关注。": "",
    "方法：研究使用了五种最新的压缩技术对三种领先的大型语言模型进行了评估，覆盖了八个可信度维度，通过实验揭示了压缩与可信度之间复杂的相互作用。": "",
    "结果：量化是目前在同时实现效率和可信度方面比剪枝更有效的方法。例如，4位量化的模型保持了原始模型的可信度，而模型剪枝即使在50%的稀疏度下也会显著降低可信度。适度位宽的量化可能意外地改善某些可信度维度，如伦理和公平性。相反，极端量化到非常低的位水平（3位）往往会显著降低可信度。这些增加的风险仅通过观察良性性能是无法发现的，从而在实践中强制要求进行全面的可信度评估。": "",
    "动机": "",
    "方法": "",
    "结果": ""
  },
  "summary_cn": "压缩高性能的大型语言模型（LLMs）已成为资源高效推理的首选策略。尽管最新水平（SoTA）的压缩方法在保持良性任务性能方面取得了令人瞩目的进步，但在安全性和可信度方面的潜在风险在很大程度上被忽视了。本研究首次对三种领先的LLMs使用了五种最新的压缩技术，从八个可信度维度进行了全面评估。我们的实验突显了压缩与可信度之间错综复杂的相互作用，并揭示了一些有趣的模式。我们发现，量化目前是一种比剪枝更有效的方法，能够在实现效率和可信度的同时取得进展。例如，一个4位量化的模型保留了其原始版本的可信度，但模型剪枝显著降低了可信度，即使在50%稀疏度的情况下也是如此。此外，在适度的比特范围内使用量化意外地改善了某些可信度维度，如伦理和公平性。相反，极端量化到非常低的比特水平（3位）往往会显著降低可信度。这种增加的风险仅通过观察良性性能是无法发现的，反过来，这要求在实践中进行全面的可信度评估。这些发现最终为同时实现LLMs的高实用性、效率和可信度提供了实用的建议。模型和代码可在 https://decoding-comp-trust.github.io/ 上获取。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"Machine Learning\",\n  \"子领域\": \"Natural Language Processing (NLP)\",\n  \"研究焦点\": \"Large Language Models (LLMs) 压缩\",\n  \"关键词\": [\n    \"压缩技术\",\n    \"资源效率\",\n    \"安全性\",\n    \"可信度\",\n    \"量化\",\n    \"剪枝\",\n    \"效率与可信度的平衡\"\n  ],\n  \"研究成果\": {\n    \"发现\": [\n      \"量化比剪枝更能有效同时实现效率和可信度\",\n      \"4位量化模型能保持原始模型的可信度\",\n      \"模型剪枝会显著降低可信度\",\n      \"适度量化能提升某些可信度维度，如伦理和公平性\",\n      \"极端量化至低比特水平（3比特）会显著降低可信度\"\n    ],\n    \"建议\": \"实践中应进行全面的可信度评估\"\n  },\n  \"数据集\": \"未提及\",\n  \"方法\": \"评估三种领先的LLMs使用五种SoTA压缩技术\",\n  \"评估维度\": \"八个可信度维度\",\n  \"资源\": {\n    \"模型和代码\": \"https://decoding-comp-trust.github.io/\"\n  }\n}\n```",
  "tag_info": {
    "主要领域": "Machine Learning",
    "子领域": "Natural Language Processing (NLP)",
    "研究焦点": "Large Language Models (LLMs) 压缩",
    "关键词": [
      "压缩技术",
      "资源效率",
      "安全性",
      "可信度",
      "量化",
      "剪枝",
      "效率与可信度的平衡"
    ],
    "研究成果": {
      "发现": [
        "量化比剪枝更能有效同时实现效率和可信度",
        "4位量化模型能保持原始模型的可信度",
        "模型剪枝会显著降低可信度",
        "适度量化能提升某些可信度维度，如伦理和公平性",
        "极端量化至低比特水平（3比特）会显著降低可信度"
      ],
      "建议": "实践中应进行全面的可信度评估"
    },
    "数据集": "未提及",
    "方法": "评估三种领先的LLMs使用五种SoTA压缩技术",
    "评估维度": "八个可信度维度",
    "资源": {
      "模型和代码": "https://decoding-comp-trust.github.io/"
    },
    "标签": []
  }
}
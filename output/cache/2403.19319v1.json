{
  "id": "2403.19319v1",
  "title": "Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field Representation and Generation",
  "pdf_url": "http://arxiv.org/pdf/2403.19319v1",
  "raw_tldr": "动机：为了改进3D生成任务中的辐射场表示，解决现有方法中由于遮挡或拟合不足导致的人为缺陷问题。\n方法：提出了Mesh2NeRF方法，通过直接从3D网格中获取真实的辐射场，使用占据函数描述密度场并定义表面厚度，通过反射函数确定视图依赖的颜色，同时考虑网格和环境光照。\n结果：Mesh2NeRF在多个任务中验证了其有效性，在ABO数据集上单场景表示的视图合成中PSNR提高了3.12dB，在ShapeNet Cars的单视图条件生成中PSNR提升了0.69，在Objaverse Mugs的无条件生成中显著改进了从NeRF中提取网格的性能。",
  "tldr": {
    "动机：为了改进3D生成任务中的辐射场表示，解决现有方法中由于遮挡或拟合不足导致的人为缺陷问题。": "",
    "方法：提出了Mesh2NeRF方法，通过直接从3D网格中获取真实的辐射场，使用占据函数描述密度场并定义表面厚度，通过反射函数确定视图依赖的颜色，同时考虑网格和环境光照。": "",
    "结果：Mesh2NeRF在多个任务中验证了其有效性，在ABO数据集上单场景表示的视图合成中PSNR提高了3.12dB，在ShapeNet Cars的单视图条件生成中PSNR提升了0.69，在Objaverse Mugs的无条件生成中显著改进了从NeRF中提取网格的性能。": "",
    "动机": "",
    "方法": "",
    "结果": ""
  },
  "summary_cn": "我们提出了Mesh2NeRF，这是一种从纹理化的网格中推导出用于3D生成任务的真实辐射场的方法。许多3D生成方法在训练时将3D场景表示为辐射场。它们的真实辐射场通常通过从大规模合成3D数据集中的多视角渲染来拟合，这往往由于遮挡或欠拟合问题而导致伪影。在Mesh2NeRF中，我们提出了一个分析解决方案，可以直接从3D网格中获得真实辐射场，通过一个具有定义表面厚度的占用函数来表征密度场，并通过一个同时考虑网格和环境照明的反射函数来确定视角依赖的颜色。Mesh2NeRF提取的准确辐射场为训练生成性NeRF和单一场景表示提供了直接监督。我们在各种任务中验证了Mesh2NeRF的有效性，在ABO数据集上的单一场景表示的视图合成中实现了显著的3.12dB的PSNR改进，在ShapeNet Cars的单视角条件生成中实现了0.69的PSNR增强，并且在Objaverse Mugs的无条件生成中显著改进了从NeRF中提取网格的能力。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"CV\",\n  \"标签\": [\"3D生成\", \"NeRF\", \"辐射场\", \"纹理网格\", \"多视角渲染\", \"视图合成\", \"场景表示\", \"物体提取\", \"PSNR\", \"ShapeNet Cars\", \"Objaverse Mugs\", \"Mesh2NeRF\"]\n}\n```",
  "tag_info": {
    "主要领域": "CV",
    "标签": [
      "3D生成",
      "NeRF",
      "辐射场",
      "纹理网格",
      "多视角渲染",
      "视图合成",
      "场景表示",
      "物体提取",
      "PSNR",
      "ShapeNet Cars",
      "Objaverse Mugs",
      "Mesh2NeRF"
    ]
  }
}
{
  "id": "2403.02775v1",
  "title": "EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs",
  "pdf_url": "http://arxiv.org/pdf/2403.02775v1",
  "raw_tldr": "动机\t大型语言模型（LLMs）虽然在各种任务上表现出色，但其昂贵的计算成本和高内存需求限制了其部署。现有的量化模型大多通过使用少量训练数据进行校准，可能影响量化后的LLMs对未知情况和任务的泛化能力。因此，本文探讨了一个重要问题：是否可以设计一种数据独立的量化方法，以保证LLMs的泛化性能？\n方法\tEasyQuant是一种无需训练、数据独立的仅针对权重的量化算法。通过保留少于1%的异常值不变，并优化量化范围来减少重构误差，从而降低量化误差。EasyQuant可以并行实现，即使对于超过100B的LLMs，也能在几分钟内完成量化模型的获取。\n结果\tEasyQuant达到了与原始模型相当的性能，几乎无损的量化性能，并且其算法的运行速度是依赖数据方法的10倍以上。由于EasyQuant不依赖任何训练数据，因此量化后的LLMs的泛化性能得到了安全保证。",
  "tldr": {
    "动机": "大型语言模型（LLMs）虽然在各种任务上表现出色，但其昂贵的计算成本和高内存需求限制了其部署。现有的量化模型大多通过使用少量训练数据进行校准，可能影响量化后的LLMs对未知情况和任务的泛化能力。因此，本文探讨了一个重要问题：是否可以设计一种数据独立的量化方法，以保证LLMs的泛化性能？",
    "方法": "EasyQuant是一种无需训练、数据独立的仅针对权重的量化算法。通过保留少于1%的异常值不变，并优化量化范围来减少重构误差，从而降低量化误差。EasyQuant可以并行实现，即使对于超过100B的LLMs，也能在几分钟内完成量化模型的获取。",
    "结果": "EasyQuant达到了与原始模型相当的性能，几乎无损的量化性能，并且其算法的运行速度是依赖数据方法的10倍以上。由于EasyQuant不依赖任何训练数据，因此量化后的LLMs的泛化性能得到了安全保证。"
  },
  "summary_cn": "大型语言模型（LLMs）已被证明在各种任务中远超传统方法。然而，它们昂贵的计算成本和高内存需求限制了其部署。模型量化是一种有效减少这种开销的方法。问题在于，在大多数以往的工作中，量化模型是使用训练数据中的少量样本进行校准的，这可能会影响量化LLMs对未知情况和任务的泛化能力。因此，在这项工作中，我们探讨了一个重要问题：我们能否为LLMs设计一种与数据无关的量化方法来保证其泛化性能？在这项工作中，我们提出了EasyQuant，一种无需训练、与数据无关的仅限权重的量化算法。我们的观察表明，两个因素：权重中的异常值和量化范围，对于减少量化误差至关重要。因此，在EasyQuant中，我们保留了少于1%的异常值不变，并优化了量化范围以减少重构误差。通过这些方法，我们惊讶地发现EasyQuant达到了与原始模型相当的性能。由于EasyQuant不依赖于任何训练数据，因此量化LLMs的泛化性能得到了安全保证。此外，EasyQuant可以并行实现，因此即使对于超过100B的LLMs，也能在几分钟内获得量化模型。据我们所知，我们是第一个在与数据无关的设置下实现几乎无损量化性能的工作，而且我们的算法运行速度比依赖数据的方法快10倍以上。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"Machine Learning\",\n  \"标签\": [\n    \"Large Language Models\",\n    \"Model Quantization\",\n    \"Data-Independent Quantization\",\n    \"Generalization Performance\",\n    \"EasyQuant\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "Machine Learning",
    "标签": [
      "Large Language Models",
      "Model Quantization",
      "Data-Independent Quantization",
      "Generalization Performance",
      "EasyQuant"
    ]
  }
}
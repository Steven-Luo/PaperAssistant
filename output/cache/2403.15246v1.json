{
  "id": "2403.15246v1",
  "title": "FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions",
  "pdf_url": "http://arxiv.org/pdf/2403.15246v1",
  "raw_tldr": "动机\t现代大型语言模型（LLMs）能够执行长而复杂的指令，以支持多种用户任务。尽管信息检索（IR）模型将LLMs作为其架构的基础，但几乎所有这些模型仍然只接受查询作为输入，不接受指令。对于少数接受指令的最新模型，目前尚不清楚它们如何使用这些指令。\n方法\t我们介绍了我们的数据集FollowIR，它包含一个严格的指令评估基准以及一个训练集，帮助IR模型学习更好地遵循现实世界的指令。FollowIR基于TREC会议的悠久历史：正如TREC提供给人类注释者指令（也称为叙述）以确定文档相关性一样，IR模型也应能够基于这些详细指令理解和决定相关性。我们的评估基准从三个深度评审的TREC集合开始，并更改注释者的指令，重新注释相关文档。通过这一过程，我们可以通过一种新的成对评估框架，衡量IR模型遵循指令的能力。\n结果\t我们的结果表明，现有的检索模型未能正确使用指令，使用它们进行基本关键词搜索，并且难以理解长形式信息。然而，我们展示了IR模型有可能学会遵循复杂的指令：我们的新FollowIR-7B模型在我们的训练集上进行微调后，有显著的改进（超过13%）。",
  "tldr": {
    "动机": "现代大型语言模型（LLMs）能够执行长而复杂的指令，以支持多种用户任务。尽管信息检索（IR）模型将LLMs作为其架构的基础，但几乎所有这些模型仍然只接受查询作为输入，不接受指令。对于少数接受指令的最新模型，目前尚不清楚它们如何使用这些指令。",
    "方法": "我们介绍了我们的数据集FollowIR，它包含一个严格的指令评估基准以及一个训练集，帮助IR模型学习更好地遵循现实世界的指令。FollowIR基于TREC会议的悠久历史：正如TREC提供给人类注释者指令（也称为叙述）以确定文档相关性一样，IR模型也应能够基于这些详细指令理解和决定相关性。我们的评估基准从三个深度评审的TREC集合开始，并更改注释者的指令，重新注释相关文档。通过这一过程，我们可以通过一种新的成对评估框架，衡量IR模型遵循指令的能力。",
    "结果": "我们的结果表明，现有的检索模型未能正确使用指令，使用它们进行基本关键词搜索，并且难以理解长形式信息。然而，我们展示了IR模型有可能学会遵循复杂的指令：我们的新FollowIR-7B模型在我们的训练集上进行微调后，有显著的改进（超过13%）。"
  },
  "summary_cn": "现代大型语言模型（LLMs）能够遵循长且复杂的指令，以支持多种用户任务。然而，尽管信息检索（IR）模型将LLMs作为其架构的支柱，几乎所有这些模型仍然只以查询作为输入，不包含指令。对于少数最近确实采用指令的模型，其使用指令的方式尚不清楚。我们介绍了我们的数据集FollowIR，该数据集包含一个严格的指令评估基准测试以及一个训练集，帮助IR模型学习更好地遵循现实世界的指令。FollowIR基于TREC会议的悠久历史：正如TREC为人类注释者提供指令（也称为叙述）以确定文档相关性，IR模型也应能够理解并根据这些详细指令判断相关性。我们的评估基准测试从三个深度评判的TREC收藏集开始，并更改了注释者的指令，重新注释相关文档。通过这一过程，我们可以通过一个新的成对评估框架，衡量IR模型遵循指令的能力。我们的结果表明，现有的检索模型未能正确使用指令，仅将它们用于基本关键词，并且难以理解长形信息。然而，我们展示了IR模型有可能学会遵循复杂指令：我们的新FollowIR-7B模型在我们的训练集上进行微调后，有显著改进（超过13%）。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"Information Retrieval\",\n  \"标签\": [\n    \"Large Language Models\",\n    \"Information Retrieval\",\n    \"Dataset\",\n    \"Evaluation Benchmark\",\n    \"TREC\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "Information Retrieval",
    "标签": [
      "Large Language Models",
      "Information Retrieval",
      "Dataset",
      "Evaluation Benchmark",
      "TREC"
    ]
  }
}
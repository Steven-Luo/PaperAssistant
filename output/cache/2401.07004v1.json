{
  "id": "2401.07004v1",
  "title": "Extending LLMs' Context Window with 100 Samples",
  "pdf_url": "http://arxiv.org/pdf/2401.07004v1",
  "raw_tldr": "动机\t大型语言模型（LLMs）在其预训练的上下文窗口之外的外推能力有限，限制了它们在具有较长输入的下游任务中的应用。现有研究试图通过修改旋转位置嵌入（RoPE）来扩展LLMs的上下文窗口，但先前的工作如位置插值（PI）和YaRN资源密集且缺乏比较实验来评估其适用性。\n方法\t本工作通过调整RoPE的基频和缩放注意力对数来引入一种新的RoPE扩展方法，帮助LLMs有效适应更大的上下文窗口，并通过在不同上下文窗口大小的各种要求高上下文任务上验证我们方法的优越性，包括微调性能和鲁棒性。\n结果\t我们的方法使LLaMA-2-7B-Chat的上下文窗口扩展到16,384，仅使用100个样本和6个训练步骤，展示了非凡的效率。此外，我们还探讨了数据组成和训练课程如何影响特定下游任务的上下文窗口扩展，建议以长度较长的对话为起点进行LLMs的微调。",
  "tldr": {
    "动机": "大型语言模型（LLMs）在其预训练的上下文窗口之外的外推能力有限，限制了它们在具有较长输入的下游任务中的应用。现有研究试图通过修改旋转位置嵌入（RoPE）来扩展LLMs的上下文窗口，但先前的工作如位置插值（PI）和YaRN资源密集且缺乏比较实验来评估其适用性。",
    "方法": "本工作通过调整RoPE的基频和缩放注意力对数来引入一种新的RoPE扩展方法，帮助LLMs有效适应更大的上下文窗口，并通过在不同上下文窗口大小的各种要求高上下文任务上验证我们方法的优越性，包括微调性能和鲁棒性。",
    "结果": "我们的方法使LLaMA-2-7B-Chat的上下文窗口扩展到16,384，仅使用100个样本和6个训练步骤，展示了非凡的效率。此外，我们还探讨了数据组成和训练课程如何影响特定下游任务的上下文窗口扩展，建议以长度较长的对话为起点进行LLMs的微调。"
  },
  "summary_cn": "大型语言模型（LLMs）被认为在超出其预训练上下文窗口的范围内具有有限的外推能力，这限制了它们在输入较长的下游任务中的应用。最近的研究试图通过修改旋转位置嵌入（RoPE），这是一种受到LLaMA、PaLM和GPT-NeoX等知名LLMs采用的流行位置编码方法，来扩展LLMs的上下文窗口。然而，像位置插值（PI）和YaRN这样的先前工作资源密集且缺乏比较实验来评估它们的适用性。在这项工作中，我们识别了LLMs的注意力熵（即注意力得分的信息熵）需要保持稳定的内在需求，并引入了一种对RoPE的新颖扩展，该扩展结合调整RoPE的基频和缩放注意力对数，以帮助LLMs高效适应更大的上下文窗口。我们在不同上下文窗口大小的各种要求上下文的任务上，验证了我们方法在微调性能和鲁棒性方面的优越性。值得注意的是，我们的方法仅使用100个样本和6个训练步骤，就将LLaMA-2-7B-Chat的上下文窗口扩展到了16,384，展示了非凡的效率。最后，我们还探讨了数据组成和训练课程如何影响特定下游任务的上下文窗口扩展，建议以长度较长的对话微调LLMs作为一个良好的起点。我们在https://github.com/GAIR-NLP/Entropy-ABF上发布了我们的代码和SFT数据。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"Large Language Models\",\n    \"Rotary Position Embedding\",\n    \"Attention Entropy\",\n    \"Context Window Extension\",\n    \"Fine-tuning Performance\",\n    \"Robustness\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "Large Language Models",
      "Rotary Position Embedding",
      "Attention Entropy",
      "Context Window Extension",
      "Fine-tuning Performance",
      "Robustness"
    ]
  }
}
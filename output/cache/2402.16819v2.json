{
  "id": "2402.16819v2",
  "raw_tldr": "动机\t\\t引入Nemotron-4 15B，一个基于8万亿文本token训练的15亿参数的大型多语言模型，旨在提升多语言和编码任务的性能。\n方法\t\\t通过在英语、多语言和编码任务上的评估来展示Nemotron-4 15B的性能。\n结果\t\\tNemotron-4 15B在7个下游评估领域中的4个上超越了所有现有的同等规模的开放模型，并在剩余领域中与领先的开放模型表现竞争性；特别是在多语言能力上，Nemotron-4 15B展现了所有同等规模模型中最好的性能，甚至超过了体量是其四倍且专门针对多语言任务的模型。",
  "tldr": {
    "动机": "\\t引入Nemotron-4 15B，一个基于8万亿文本token训练的15亿参数的大型多语言模型，旨在提升多语言和编码任务的性能。",
    "方法": "\\t通过在英语、多语言和编码任务上的评估来展示Nemotron-4 15B的性能。",
    "结果": "\\tNemotron-4 15B在7个下游评估领域中的4个上超越了所有现有的同等规模的开放模型，并在剩余领域中与领先的开放模型表现竞争性；特别是在多语言能力上，Nemotron-4 15B展现了所有同等规模模型中最好的性能，甚至超过了体量是其四倍且专门针对多语言任务的模型。"
  },
  "summary_cn": "我们介绍了Nemotron-4 15B，这是一个拥有150亿参数的大型多语言语言模型，它在8万亿文本标记上接受了训练。Nemotron-4 15B在英语、多语言和编码任务上的表现都非常出色：在7个下游评估领域中的4个上，它超越了所有现有的同等规模的开放模型，并且在剩余的领域中与领先的开放模型表现竞争。特别是，Nemotron-4 15B展现了所有同等规模模型中最佳的多语言能力，甚至超过了体积是其四倍多的模型以及那些专门为多语言任务设计的模型。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"多语言模型\",\n    \"大型语言模型\",\n    \"Nemotron-4 15B\",\n    \"性能评估\",\n    \"编程任务\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "多语言模型",
      "大型语言模型",
      "Nemotron-4 15B",
      "性能评估",
      "编程任务"
    ]
  }
}
{
  "id": "2402.14797v1",
  "raw_tldr": "动机#当前生成图像的模型表现出了卓越的质量和多样性。受此启发，研究社区尝试将这些模型用于生成视频。然而，由于视频内容高度冗余，直接将图像模型的进步应用于视频生成领域会降低运动保真度、视觉质量并影响可扩展性。\n\n方法#我们构建了Snap Video，一个以视频为首的模型，系统性地解决这些挑战。首先，我们扩展了EDM框架，使其能够考虑空间和时间上冗余的像素，并自然支持视频生成。其次，我们发现U-Net在生成视频时扩展性差，需要大量的计算开销。因此，我们提出了一种新的基于变换器的架构，训练速度是U-Net的3.31倍（推理速度约快4.5倍）。\n\n结果#我们的模型是首个有效训练的拥有数十亿参数的文本到视频模型，达到了多个基准测试的最先进结果，并生成了具有显著更高质量、时间一致性和运动复杂性的视频。用户研究显示，我们的模型相较于最新方法获得了压倒性的青睐。",
  "summary_cn": "当代用于生成图像的模型展现出了卓越的质量和多功能性。受到这些优势的影响，研究社区将它们重新用途于生成视频。由于视频内容高度冗余，我们认为，简单地将图像模型的进步应用到视频生成领域，会降低运动保真度、视觉质量并损害可扩展性。在这项工作中，我们构建了Snap Video，一个以视频为首要考虑的模型，系统性地解决了这些挑战。为此，我们首先扩展了EDM框架，以考虑空间上和时间上冗余的像素，并自然支持视频生成。其次，我们展示了U-Net——图像生成背后的主力——在生成视频时扩展性差，需要显著的计算开销。因此，我们提出了一种新的基于变换器的架构，训练速度比U-Net快3.31倍（推理速度约快4.5倍）。这使我们能够首次高效训练出一个具有数十亿参数的文本到视频模型，在多个基准测试上达到了最先进的结果，并生成了具有显著更高质量、时间一致性和运动复杂性的视频。用户研究显示，与最新方法相比，我们的模型受到了压倒性的青睐。请访问我们的网站 https://snap-research.github.io/snapvideo/。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"多模态\",\n  \"标签\": [\n    \"视频生成\",\n    \"图像模型\",\n    \"EDM框架\",\n    \"U-Net\",\n    \"Transformer\",\n    \"文本到视频\",\n    \"计算效率\",\n    \"状态艺术结果\",\n    \"用户研究\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "多模态",
    "标签": [
      "视频生成",
      "图像模型",
      "EDM框架",
      "U-Net",
      "Transformer",
      "文本到视频",
      "计算效率",
      "状态艺术结果",
      "用户研究"
    ]
  },
  "tldr": {
    "动机": "当前生成图像的模型表现出了卓越的质量和多样性。受此启发，研究社区尝试将这些模型用于生成视频。然而，由于视频内容高度冗余，直接将图像模型的进步应用于视频生成领域会降低运动保真度、视觉质量并影响可扩展性。",
    "方法": "我们构建了Snap Video，一个以视频为首的模型，系统性地解决这些挑战。首先，我们扩展了EDM框架，使其能够考虑空间和时间上冗余的像素，并自然支持视频生成。其次，我们发现U-Net在生成视频时扩展性差，需要大量的计算开销。因此，我们提出了一种新的基于变换器的架构，训练速度是U-Net的3.31倍（推理速度约快4.5倍）。",
    "结果": "我们的模型是首个有效训练的拥有数十亿参数的文本到视频模型，达到了多个基准测试的最先进结果，并生成了具有显著更高质量、时间一致性和运动复杂性的视频。用户研究显示，我们的模型相较于最新方法获得了压倒性的青睐。"
  }
}
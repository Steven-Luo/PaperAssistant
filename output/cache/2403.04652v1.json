{
  "id": "2403.04652v1",
  "title": "Yi: Open Foundation Models by 01.AI",
  "pdf_url": "http://arxiv.org/pdf/2403.04652v1",
  "raw_tldr": "动机\t我们引入了Yi模型家族，这是一系列展示出强大多维能力的语言和多模态模型，旨在通过扩展预训练语言模型和改进数据工程，提高模型在多个基准测试和应用场景中的性能。\n方法\t基于6B和34B的预训练语言模型，通过数据工程优化、与视觉变换器编码器结合、以及轻量级持续预训练等方法，扩展到聊天模型、200K长上下文模型、深度升级模型和视觉-语言模型。\n结果\t基础模型在多个基准测试（如MMLU）上表现出色，经过微调的聊天模型在主要评估平台（如AlpacaEval和Chatbot Arena）上获得了高人类偏好率，通过持续预训练深度扩展和结合视觉-语言训练，实现了强大的“针尖上的麦穗”检索性能，并认为通过使用优化数据进一步扩大模型参数将导致更强大的前沿模型。",
  "tldr": {
    "动机": "我们引入了Yi模型家族，这是一系列展示出强大多维能力的语言和多模态模型，旨在通过扩展预训练语言模型和改进数据工程，提高模型在多个基准测试和应用场景中的性能。",
    "方法": "基于6B和34B的预训练语言模型，通过数据工程优化、与视觉变换器编码器结合、以及轻量级持续预训练等方法，扩展到聊天模型、200K长上下文模型、深度升级模型和视觉-语言模型。",
    "结果": "基础模型在多个基准测试（如MMLU）上表现出色，经过微调的聊天模型在主要评估平台（如AlpacaEval和Chatbot Arena）上获得了高人类偏好率，通过持续预训练深度扩展和结合视觉-语言训练，实现了强大的“针尖上的麦穗”检索性能，并认为通过使用优化数据进一步扩大模型参数将导致更强大的前沿模型。"
  },
  "summary_cn": "我们介绍了易模型家族，这是一系列展示出强大多维能力的语言和多模态模型。易模型家族基于6B和34B的预训练语言模型，然后我们将它们扩展到聊天模型、20万长上下文模型、深度升级模型和视觉-语言模型。我们的基础模型在如MMLU等一系列基准测试上取得了强大的性能，而我们的微调聊天模型在AlpacaEval和Chatbot Arena等主要评估平台上展现出了强烈的人类偏好率。基于我们可扩展的超级计算基础设施和经典的变换器架构，我们认为易模型的性能主要归功于我们的数据工程努力带来的数据质量。对于预训练，我们使用级联数据去重和质量过滤管道构建了3.1万亿个英文和中文语料库。对于微调，我们在多次迭代中打磨小规模（少于1万）的指令数据集，以确保每个实例都直接经过我们的机器学习工程师的验证。对于视觉-语言，我们将聊天语言模型与视觉变换器编码器结合，并训练模型将视觉表示与语言模型的语义空间对齐。我们进一步通过轻量级持续预训练将上下文长度扩展到20万，并展示出强大的大海捞针式检索性能。我们展示了通过持续预训练扩展预训练检查点的深度进一步提升了性能。我们相信，鉴于我们当前的结果，继续使用经过彻底优化的数据扩大模型参数将导致更强大的前沿模型。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": [\"NLP\", \"多模态\", \"Machine Learning\"],\n  \"标签\": [\n    \"语言模型\",\n    \"多模态模型\",\n    \"预训练模型\",\n    \"聊天模型\",\n    \"长上下文模型\",\n    \"视觉-语言模型\",\n    \"数据工程\",\n    \"transformer架构\",\n    \"细调\",\n    \"性能优化\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "语言模型",
      "多模态模型",
      "预训练模型",
      "聊天模型",
      "长上下文模型",
      "视觉-语言模型",
      "数据工程",
      "transformer架构",
      "细调",
      "性能优化"
    ]
  }
}
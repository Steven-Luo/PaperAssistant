{
  "id": "2402.10171v1",
  "title": "Data Engineering for Scaling Language Models to 128K Context",
  "pdf_url": "http://arxiv.org/pdf/2402.10171v1",
  "raw_tldr": "动机\t我们研究了将语言模型的上下文长度扩展到128K的持续预训练方法，重点关注数据工程。我们假设长上下文建模，特别是在任意输入位置利用信息的能力，主要是通过大规模预训练获得的，并且这种能力可以通过在适当的数据混合上进行轻量级持续预训练，轻松扩展到训练期间未见过的更长上下文（例如，从4K扩展到128K）。\n方法\t我们调查了持续预训练的数据的数量和质量：（1）就数量而言，我们展示了500百万到50亿个令牌足以使模型能够在128K上下文中的任何位置检索信息；（2）就质量而言，我们的结果同样强调了领域平衡和长度上采样的重要性。具体来说，我们发现，像书籍这样的某些领域中对较长数据的简单上采样（现有工作的常见做法）会导致次优性能，而平衡的领域混合很重要。\n结果\t我们证明了在此类数据上对全模型进行1B-5B令牌的持续预训练是一种有效且经济的策略，用于将语言模型的上下文长度扩展到128K。我们的方法优于强大的开源长上下文模型，并缩小了与前沿模型（如GPT-4 128K）的差距。",
  "tldr": {
    "动机": "我们研究了将语言模型的上下文长度扩展到128K的持续预训练方法，重点关注数据工程。我们假设长上下文建模，特别是在任意输入位置利用信息的能力，主要是通过大规模预训练获得的，并且这种能力可以通过在适当的数据混合上进行轻量级持续预训练，轻松扩展到训练期间未见过的更长上下文（例如，从4K扩展到128K）。",
    "方法": "我们调查了持续预训练的数据的数量和质量：（1）就数量而言，我们展示了500百万到50亿个令牌足以使模型能够在128K上下文中的任何位置检索信息；（2）就质量而言，我们的结果同样强调了领域平衡和长度上采样的重要性。具体来说，我们发现，像书籍这样的某些领域中对较长数据的简单上采样（现有工作的常见做法）会导致次优性能，而平衡的领域混合很重要。",
    "结果": "我们证明了在此类数据上对全模型进行1B-5B令牌的持续预训练是一种有效且经济的策略，用于将语言模型的上下文长度扩展到128K。我们的方法优于强大的开源长上下文模型，并缩小了与前沿模型（如GPT-4 128K）的差距。"
  },
  "summary_cn": "我们研究了用于扩展语言模型上下文长度至128K的持续预训练方法，重点关注数据工程。我们假设长上下文建模，特别是\\textit{利用任意输入位置的信息的能力}，主要是通过大规模预训练已经获得的能力，并且这种能力可以通过在适当的数据混合上进行轻量级持续预训练，轻松扩展到训练期间未见过的大得多的上下文长度（例如，从4K扩展到128K）。我们调查了持续预训练的数据\\textit{数量}和\\textit{质量}：（1）对于数量，我们展示了5亿到50亿个令牌足以使模型能够在128K上下文内的任何位置检索信息；（2）对于质量，我们的结果同样强调了\\textit{领域平衡}和\\textit{长度上采样}的重要性。具体来说，我们发现，像书籍这样的某些领域中，对较长数据进行简单上采样（现有工作的常见做法）会导致次优表现，而平衡的领域混合是重要的。我们证明了在此类数据上对整个模型进行1B-5B令牌的持续预训练是一种有效且可行的策略，用于将语言模型的上下文长度扩展到128K。我们的方法优于强大的开源长上下文模型，并缩小了与前沿模型（如GPT-4 128K）的差距。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"长文本建模\",\n    \"持续预训练\",\n    \"数据工程\",\n    \"上下文长度扩展\",\n    \"数据量\",\n    \"数据质量\",\n    \"域平衡\",\n    \"长度上采样\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "长文本建模",
      "持续预训练",
      "数据工程",
      "上下文长度扩展",
      "数据量",
      "数据质量",
      "域平衡",
      "长度上采样"
    ]
  }
}
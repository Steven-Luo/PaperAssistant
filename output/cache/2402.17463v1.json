{
  "id": "2402.17463v1",
  "raw_tldr": "动机\t大型语言模型(LLMs)在输入令牌数量超过其预训练长度时，处理和生成连贯文本的能力显著减弱。鉴于对大规模模型进行长序列微调的高昂开销，我们提出了一种方法。\n方法\t提出双块注意力(DCA)机制，使Llama2 70B能够支持超过100k令牌的上下文窗口，而无需持续训练。通过将长序列的注意力计算分解为基于块的模块，DCA有效地捕获了同一块内（Intra-Chunk）和不同块之间（Inter-Chunk）令牌的相对位置信息，并与Flash Attention无缝集成。\n结果\tDCA在实际长上下文任务上的表现与微调模型相当或甚至更好。与专有模型相比，我们的无需训练的70B模型达到了gpt-3.5-16k性能的94%，表明它是一个可行的开源替代方案。",
  "tldr": {
    "动机": "大型语言模型(LLMs)在输入令牌数量超过其预训练长度时，处理和生成连贯文本的能力显著减弱。鉴于对大规模模型进行长序列微调的高昂开销，我们提出了一种方法。",
    "方法": "提出双块注意力(DCA)机制，使Llama2 70B能够支持超过100k令牌的上下文窗口，而无需持续训练。通过将长序列的注意力计算分解为基于块的模块，DCA有效地捕获了同一块内（Intra-Chunk）和不同块之间（Inter-Chunk）令牌的相对位置信息，并与Flash Attention无缝集成。",
    "结果": "DCA在实际长上下文任务上的表现与微调模型相当或甚至更好。与专有模型相比，我们的无需训练的70B模型达到了gpt-3.5-16k性能的94%，表明它是一个可行的开源替代方案。"
  },
  "summary_cn": "大型语言模型（LLMs）在输入令牌数量超过其预训练长度时，处理和生成连贯文本的能力显著减弱。鉴于对大规模模型进行长序列微调的高昂成本，我们提出了双块注意力（DCA）机制，使得Llama2 70B能够支持超过100k令牌的上下文窗口，而无需持续训练。通过将长序列的注意力计算分解为基于块的模块，DCA有效地捕获了同一块内（块内）和不同块之间（块间）令牌的相对位置信息，并且与Flash Attention无缝集成。除了其令人印象深刻的外推能力外，DCA在实际长上下文任务上的表现与微调模型相当，甚至更好。与专有模型相比，我们的无需训练的70B模型达到了gpt-3.5-16k性能的94%，表明它是一个可行的开源替代品。本工作使用的所有代码和数据已在\\url{https://github.com/HKUNLP/ChunkLlama}发布。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"Large Language Models\",\n    \"Dual Chunk Attention\",\n    \"Long-context tasks\",\n    \"Open-source\",\n    \"Performance comparison\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "Large Language Models",
      "Dual Chunk Attention",
      "Long-context tasks",
      "Open-source",
      "Performance comparison"
    ]
  }
}
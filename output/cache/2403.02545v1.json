{
  "id": "2403.02545v1",
  "title": "Wukong: Towards a Scaling Law for Large-Scale Recommendation",
  "pdf_url": "http://arxiv.org/pdf/2403.02545v1",
  "raw_tldr": "动机\t推荐模型到目前为止没有展现出类似于大型语言模型中观察到的可扩展性法则，这限制了这些模型适应越来越复杂的现实世界数据集的能力。\n方法\t提出了一种基于堆叠因子分解机的有效网络架构和一种协同的扩展策略，统称为Wukong，以在推荐领域建立扩展法则。\n结果\tWukong在六个公开数据集上的广泛评估显示，其一致性地在质量上超越了最先进的模型，并在内部的大规模数据集上评估了Wukong的可扩展性，结果显示Wukong在保持扩展法则的同时，其质量优于最先进的模型，覆盖了模型复杂度的两个数量级，扩展到超过100 Gflop或等效于GPT-3/LLaMa-2规模的总训练计算，而先前的艺术作品则做不到这一点。",
  "tldr": {
    "动机": "推荐模型到目前为止没有展现出类似于大型语言模型中观察到的可扩展性法则，这限制了这些模型适应越来越复杂的现实世界数据集的能力。",
    "方法": "提出了一种基于堆叠因子分解机的有效网络架构和一种协同的扩展策略，统称为Wukong，以在推荐领域建立扩展法则。",
    "结果": "Wukong在六个公开数据集上的广泛评估显示，其一致性地在质量上超越了最先进的模型，并在内部的大规模数据集上评估了Wukong的可扩展性，结果显示Wukong在保持扩展法则的同时，其质量优于最先进的模型，覆盖了模型复杂度的两个数量级，扩展到超过100 Gflop或等效于GPT-3/LLaMa-2规模的总训练计算，而先前的艺术作品则做不到这一点。"
  },
  "summary_cn": "缩放定律在模型质量的可持续改进中起着至关重要的作用。不幸的是，迄今为止的推荐模型并未展现出类似于大型语言模型领域观察到的那种定律，这是由于它们的扩展机制效率低下。这一限制对于将这些模型适应越来越复杂的现实世界数据集提出了重大挑战。在本文中，我们提出了一种基于堆叠因子分解机的有效网络架构，以及一种协同的扩展策略，统称为Wukong，以在推荐领域建立一个缩放定律。Wukong的独特设计使其能够仅通过更高更宽的层来捕捉多样化的、任意阶的交互。我们在六个公开数据集上进行了广泛的评估，结果表明Wukong在质量上一致优于最先进的模型。此外，我们还评估了Wukong在一个内部的大规模数据集上的可扩展性。结果显示，Wukong在质量上保持了对最先进模型的优势，同时在模型复杂度的两个数量级上保持了缩放定律，扩展到超过100 Gflop或等效于GPT-3/LLaMa-2规模的总训练计算，而先前的技术则做不到这一点。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"推荐系统\",\n  \"标签\": [\n    \"推荐系统\",\n    \"模型扩展\",\n    \"因子分解机\",\n    \"网络架构\",\n    \"性能评估\",\n    \"可扩展性\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "推荐系统",
    "标签": [
      "推荐系统",
      "模型扩展",
      "因子分解机",
      "网络架构",
      "性能评估",
      "可扩展性"
    ]
  }
}
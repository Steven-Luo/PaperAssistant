{
  "id": "2403.08763v1",
  "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
  "pdf_url": "http://arxiv.org/pdf/2403.08763v1",
  "raw_tldr": "动机\t大型语言模型（LLMs）通常在数十亿的tokens上进行预训练，一旦有新数据可用，就要重新开始训练过程。寻找一种更高效的解决方案，即持续预训练这些模型，与重新训练相比，可以节省大量计算资源。\n方法\t通过简单且可扩展的学习率（LR）重新加热、LR重新衰减和重放以前的数据的组合，足以匹配从头开始重新训练的性能，以最终损失和语言模型（LM）评估基准为衡量标准。\n结果\t在两种常用的LLM预训练数据集之间的弱但现实的分布转移（英语→英语）以及更强的分布转移（英语→德语）中，展示了这种方法的有效性，并且在10B参数LLM上，持续学习策略的性能匹配了重新训练的基线，仅使用了一小部分计算资源。",
  "tldr": {
    "动机": "大型语言模型（LLMs）通常在数十亿的tokens上进行预训练，一旦有新数据可用，就要重新开始训练过程。寻找一种更高效的解决方案，即持续预训练这些模型，与重新训练相比，可以节省大量计算资源。",
    "方法": "通过简单且可扩展的学习率（LR）重新加热、LR重新衰减和重放以前的数据的组合，足以匹配从头开始重新训练的性能，以最终损失和语言模型（LM）评估基准为衡量标准。",
    "结果": "在两种常用的LLM预训练数据集之间的弱但现实的分布转移（英语→英语）以及更强的分布转移（英语→德语）中，展示了这种方法的有效性，并且在10B参数LLM上，持续学习策略的性能匹配了重新训练的基线，仅使用了一小部分计算资源。"
  },
  "summary_cn": "大型语言模型（LLMs）通常会在数十亿个令牌上进行预训练，一旦有新数据可用，就会重新开始这一过程。一个更高效的解决方案是持续地对这些模型进行预训练，与重新训练相比，这样可以节省大量的计算资源。然而，新数据引起的分布偏移通常会导致在先前数据上的性能下降或者对新数据的适应性差。在这项工作中，我们展示了一个简单且可扩展的组合方法——学习率（LR）重新加热、LR重新衰减以及先前数据的重放，足以达到与从头开始在所有可用数据上重新训练的性能相匹配，通过最终损失和语言模型（LM）评估基准来衡量。具体来说，我们展示了这一点，对于两个常用的LLM预训练数据集之间的弱但现实的分布偏移（英语$\\rightarrow$英语）以及更强的分布偏移（英语$\\rightarrow$德语），在具有大量数据集大小（数千亿个令牌）的$405$M参数模型规模上。选择弱但现实的偏移进行更大规模的实验，我们还发现，我们的持续学习策略对于一个10B参数的LLM匹配了重新训练的基线。我们的结果表明，LLMs可以通过简单且可扩展的持续学习策略成功更新，仅使用一小部分计算资源即可匹配重新训练的基线。最后，受到先前工作的启发，我们提出了替代余弦学习率计划的方法，这些方法有助于规避由LR重新加热引起的遗忘，并且不受固定令牌预算的限制。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"Machine Learning\",\n  \"标签\": [\n    \"Large Language Models\",\n    \"Continual Learning\",\n    \"Learning Rate Scheduling\",\n    \"Distribution Shift\",\n    \"Model Updating\",\n    \"Efficiency\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "Machine Learning",
    "标签": [
      "Large Language Models",
      "Continual Learning",
      "Learning Rate Scheduling",
      "Distribution Shift",
      "Model Updating",
      "Efficiency"
    ]
  }
}
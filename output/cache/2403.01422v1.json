{
  "id": "2403.01422v1",
  "title": "MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies",
  "pdf_url": "http://arxiv.org/pdf/2403.01422v1",
  "raw_tldr": "动机\t当前多模态模型在分析长格式视频（如电影）时表现不佳，主要障碍是高质量、多样化视频数据的缺乏以及收集或注释此类数据所需的密集工作。\n方法\t提出了一种名为MovieLLM的新框架，利用GPT-4和文本到图像模型的力量，生成详细的剧本和相应的视觉内容，以创造合成的、高质量的长视频数据。\n结果\tMovieLLM生成的数据显著提高了多模态模型在理解复杂视频叙事方面的性能，克服了现有数据集在稀缺性和偏见方面的限制。",
  "tldr": {
    "动机": "当前多模态模型在分析长格式视频（如电影）时表现不佳，主要障碍是高质量、多样化视频数据的缺乏以及收集或注释此类数据所需的密集工作。",
    "方法": "提出了一种名为MovieLLM的新框架，利用GPT-4和文本到图像模型的力量，生成详细的剧本和相应的视觉内容，以创造合成的、高质量的长视频数据。",
    "结果": "MovieLLM生成的数据显著提高了多模态模型在理解复杂视频叙事方面的性能，克服了现有数据集在稀缺性和偏见方面的限制。"
  },
  "summary_cn": "多模态模型的发展标志着机器理解视频方面的一大步进。这些模型在分析短视频片段方面显示出了潜力。然而，当涉及到更长格式的视频，如电影时，它们往往表现不佳。主要障碍是缺乏高质量、多样化的视频数据，以及收集或标注此类数据所需的大量工作。面对这些挑战，我们提出了MovieLLM，一个旨在为长视频创建合成、高质量数据的新颖框架。该框架利用GPT-4和文本到图像模型的力量，生成详细的剧本和相应的视觉内容。我们的方法以其灵活性和可扩展性脱颖而出，使其成为传统数据收集方法的优越替代品。我们广泛的实验验证了，由MovieLLM产生的数据显著提高了多模态模型在理解复杂视频叙事方面的性能，克服了现有数据集在稀缺性和偏见方面的限制。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"多模态\",\n  \"标签\": [\"多模态模型\", \"长视频理解\", \"合成数据\", \"GPT-4\", \"文本到图像模型\", \"数据集偏差与稀缺性\"]\n}\n```",
  "tag_info": {
    "主要领域": "多模态",
    "标签": [
      "多模态模型",
      "长视频理解",
      "合成数据",
      "GPT-4",
      "文本到图像模型",
      "数据集偏差与稀缺性"
    ]
  }
}
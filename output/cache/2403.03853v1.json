{
  "id": "2403.03853v1",
  "title": "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect",
  "pdf_url": "http://arxiv.org/pdf/2403.03853v1",
  "raw_tldr": "动机\t随着大型语言模型（LLMs）性能的不断提升，它们的规模显著增加，当前的LLMs包含数十亿甚至数万亿个参数。然而，在本研究中，我们发现LLMs的许多层展现出高度相似性，且某些层在网络功能中的作用可以忽略不计。  \n方法\t基于这一观察，我们定义了一个称为块影响（BI）的度量标准来衡量LLMs中每一层的重要性，并提出了一种直接的剪枝方法：层移除，即根据它们的BI分数直接删除LLMs中的冗余层。  \n结果\t实验表明，我们的方法（称为ShortGPT）在模型剪枝方面显著优于之前的最先进（SOTA）方法。此外，ShortGPT与类似量化的方法正交，能够进一步减少参数和计算量。通过简单的层移除而非更复杂的剪枝技术实现更好的结果，表明模型架构中存在高度的冗余。  ",
  "tldr": {
    "动机": "随着大型语言模型（LLMs）性能的不断提升，它们的规模显著增加，当前的LLMs包含数十亿甚至数万亿个参数。然而，在本研究中，我们发现LLMs的许多层展现出高度相似性，且某些层在网络功能中的作用可以忽略不计。",
    "方法": "基于这一观察，我们定义了一个称为块影响（BI）的度量标准来衡量LLMs中每一层的重要性，并提出了一种直接的剪枝方法：层移除，即根据它们的BI分数直接删除LLMs中的冗余层。",
    "结果": "实验表明，我们的方法（称为ShortGPT）在模型剪枝方面显著优于之前的最先进（SOTA）方法。此外，ShortGPT与类似量化的方法正交，能够进一步减少参数和计算量。通过简单的层移除而非更复杂的剪枝技术实现更好的结果，表明模型架构中存在高度的冗余。"
  },
  "summary_cn": "随着大型语言模型（LLMs）性能的不断提升，它们的规模也显著增加，当前的LLMs包含了数十亿甚至数万亿个参数。然而，在本研究中，我们发现LLMs的许多层次表现出高度相似性，且某些层在网络功能中的作用几乎可以忽略不计。基于这一观察，我们定义了一个称为块影响力（BI）的度量标准，用以衡量LLMs中每一层的重要性。接着，我们提出了一种直接的剪枝方法：层移除，即根据它们的BI得分直接删除LLMs中的冗余层。实验表明，我们称之为ShortGPT的方法显著优于之前最先进（SOTA）方法在模型剪枝方面的表现。此外，ShortGPT与类似量化的方法正交，能够进一步减少参数和计算量。通过简单的层移除而非更复杂的剪枝技术就能够获得更好的结果，这表明模型架构中存在高度的冗余。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"Machine Learning\",\n  \"标签\": [\"Large Language Models\", \"Model Pruning\", \"Network Functionality\", \"Block Influence\", \"ShortGPT\"]\n}\n```",
  "tag_info": {
    "主要领域": "Machine Learning",
    "标签": [
      "Large Language Models",
      "Model Pruning",
      "Network Functionality",
      "Block Influence",
      "ShortGPT"
    ]
  }
}
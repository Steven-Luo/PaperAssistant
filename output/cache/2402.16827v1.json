{
  "id": "2402.16827v1",
  "title": "A Survey on Data Selection for Language Models",
  "pdf_url": "http://arxiv.org/pdf/2402.16827v1",
  "raw_tldr": "动机\t大型语言模型近期的成功很大程度上归因于使用庞大且不断增长的文本数据集进行无监督预训练。然而，简单地在所有可用数据上训练模型可能不是最佳（或可行）的选择，因为可用文本数据的质量参差不齐。筛选数据还可以通过减少所需的训练量来降低训练模型的碳足迹和财务成本。数据选择方法旨在确定哪些候选数据点纳入训练数据集以及如何从选定的数据点中适当抽样。改进数据选择方法的承诺导致该领域的研究量迅速扩大。然而，由于深度学习主要由经验证据驱动，而且在大规模数据上进行实验成本高昂，因此很少有组织有资源进行广泛的数据选择研究。因此，有效的数据选择实践知识已经在少数几个组织中集中，其中许多组织不公开分享他们的发现和方法论。\n方法\t我们提出了一篇综述性论文，对现有的数据选择方法及相关研究领域的文献进行了全面回顾，提供了现有方法的分类体系。通过描述当前研究的现状，这项工作旨在通过为新老研究人员建立一个入口点来加速数据选择方面的进展。此外，在整篇综述中，我们关注文献中明显的空白，并在论文的结论部分提出了未来研究的有希望的方向。\n结果\t本综述旨在通过提供现有数据选择方法的分类和概述，加速数据选择领域的进展，并指出文献中的空白，提出未来研究的有希望的方向。",
  "tldr": {
    "动机": "大型语言模型近期的成功很大程度上归因于使用庞大且不断增长的文本数据集进行无监督预训练。然而，简单地在所有可用数据上训练模型可能不是最佳（或可行）的选择，因为可用文本数据的质量参差不齐。筛选数据还可以通过减少所需的训练量来降低训练模型的碳足迹和财务成本。数据选择方法旨在确定哪些候选数据点纳入训练数据集以及如何从选定的数据点中适当抽样。改进数据选择方法的承诺导致该领域的研究量迅速扩大。然而，由于深度学习主要由经验证据驱动，而且在大规模数据上进行实验成本高昂，因此很少有组织有资源进行广泛的数据选择研究。因此，有效的数据选择实践知识已经在少数几个组织中集中，其中许多组织不公开分享他们的发现和方法论。",
    "方法": "我们提出了一篇综述性论文，对现有的数据选择方法及相关研究领域的文献进行了全面回顾，提供了现有方法的分类体系。通过描述当前研究的现状，这项工作旨在通过为新老研究人员建立一个入口点来加速数据选择方面的进展。此外，在整篇综述中，我们关注文献中明显的空白，并在论文的结论部分提出了未来研究的有希望的方向。",
    "结果": "本综述旨在通过提供现有数据选择方法的分类和概述，加速数据选择领域的进展，并指出文献中的空白，提出未来研究的有希望的方向。"
  },
  "summary_cn": "大型语言模型近期成功的一个主要因素是使用庞大且不断增长的文本数据集进行无监督预训练。然而，简单地在所有可用数据上训练模型可能不是最佳（或可行）的选择，因为可用文本数据的质量可能存在差异。过滤掉数据也可以通过减少所需的训练量来减少训练模型的碳足迹和财务成本。数据选择方法旨在确定哪些候选数据点应包含在训练数据集中，以及如何从选定的数据点中适当抽样。改进数据选择方法的前景促使该领域的研究量迅速扩大。然而，由于深度学习主要由经验证据驱动，而且在大规模数据上进行实验成本高昂，因此只有少数组织拥有进行广泛数据选择研究的资源。因此，有效数据选择实践的知识已经在少数几个组织中集中，其中许多组织不公开分享他们的发现和方法论。\n\n为了缩小这一知识差距，我们提供了一篇关于数据选择方法及相关研究领域现有文献的综述，提供了现有方法的分类。通过描述当前研究的格局，本工作旨在通过为新老研究人员建立一个入口点来加速数据选择方面的进展。此外，在整篇综述中，我们关注文献中明显的空白，并在论文的结尾提出了未来研究的有前景的途径。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"Machine Learning\",\n  \"标签\": [\n    \"large language models\",\n    \"unsupervised pre-training\",\n    \"data selection methods\",\n    \"carbon footprint\",\n    \"financial costs\",\n    \"literature review\",\n    \"future research\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "Machine Learning",
    "标签": [
      "large language models",
      "unsupervised pre-training",
      "data selection methods",
      "carbon footprint",
      "financial costs",
      "literature review",
      "future research"
    ]
  }
}
{
  "id": "2402.14083v1",
  "raw_tldr": "动机#虽然变换器（Transformers）在各种应用场景中取得了巨大进展，但在解决复杂决策任务方面仍然落后于传统的符号规划器。\n\n方法#本文提出了Searchformer，一种编码器-解码器变换器模型，通过训练预测$A^*$搜索动态，并通过专家迭代进行微调，以减少搜索步骤同时生成最优计划。此外，还采用了一种训练方法，将$A^*$的搜索动态表达为在符号规划过程中向搜索树添加和移除任务状态的令牌序列。\n\n结果#Searchformer能够在93.7%的情况下优化解决之前未见过的推箱子（Sokoban）谜题，同时比标准$A^*$搜索减少了多达26.8%的搜索步骤。在迷宫导航的消融研究中，Searchformer显著优于直接预测最优计划的基线模型，模型大小小5-10倍，训练数据集小10倍，并且在解决更大和更复杂的决策任务如推箱子时，展现了更高的任务解决率和缩短的搜索动态。",
  "tldr": {
    "动机": "虽然变换器（Transformers）在各种应用场景中取得了巨大进展，但在解决复杂决策任务方面仍然落后于传统的符号规划器。",
    "方法": "本文提出了Searchformer，一种编码器-解码器变换器模型，通过训练预测$A^*$搜索动态，并通过专家迭代进行微调，以减少搜索步骤同时生成最优计划。此外，还采用了一种训练方法，将$A^*$的搜索动态表达为在符号规划过程中向搜索树添加和移除任务状态的令牌序列。",
    "结果": "Searchformer能够在93.7%的情况下优化解决之前未见过的推箱子（Sokoban）谜题，同时比标准$A^*$搜索减少了多达26.8%的搜索步骤。在迷宫导航的消融研究中，Searchformer显著优于直接预测最优计划的基线模型，模型大小小5-10倍，训练数据集小10倍，并且在解决更大和更复杂的决策任务如推箱子时，展现了更高的任务解决率和缩短的搜索动态。"
  },
  "summary_cn": "虽然Transformer模型在各种应用场景中取得了巨大进展，但在解决复杂决策任务方面，这种架构仍然落后于传统的符号规划器。在这项工作中，我们展示了如何训练Transformer来解决复杂的规划任务，并介绍了Searchformer，这是一个Transformer模型，能够在93.7%的情况下优化解决以前未见过的推箱子(Sokoban)谜题，同时使用的搜索步骤比标准的$A^*$搜索少达26.8%。Searchformer是一个经过训练的编码器-解码器Transformer模型，旨在预测$A^*$的搜索动态。然后，通过专家迭代对这个模型进行微调，以执行比$A^*$搜索更少的搜索步骤，同时仍然生成一个最优计划。在我们的训练方法中，$A^*$的搜索动态被表达为一个令牌序列，概述了在符号规划期间任务状态何时被添加和移除搜索树。在我们对迷宫导航的消融研究中，我们发现Searchformer显著优于直接预测最优计划的基线，模型大小小5-10倍，训练数据集小10倍。我们还展示了Searchformer如何扩展到更大、更复杂的决策任务，如推箱子，通过提高解决任务的百分比和缩短搜索动态来实现。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"Machine Learning\",\n  \"标签\": [\n    \"Transformers\",\n    \"Symbolic Planning\",\n    \"Sokoban Puzzles\",\n    \"Search Optimization\",\n    \"Encoder-Decoder Model\",\n    \"A* Search\",\n    \"Expert Iterations\",\n    \"Search Dynamics\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "Machine Learning",
    "标签": [
      "Transformers",
      "Symbolic Planning",
      "Sokoban Puzzles",
      "Search Optimization",
      "Encoder-Decoder Model",
      "A* Search",
      "Expert Iterations",
      "Search Dynamics"
    ]
  }
}
{
  "id": "2402.15220v1",
  "raw_tldr": "#动机\n自注意力是大型语言模型的重要组成部分，但对于长序列而言，它是推理延迟的重要来源。在多租户大型语言模型服务场景中，通过利用多个LLM请求在前缀中共享系统提示的概率，可以优化自注意力的计算和内存操作成本。\n\n#方法\n本文介绍了ChunkAttention，这是一个前缀感知的自注意力模块，能够在运行时检测多个请求中匹配的提示前缀，并在内存中共享它们的键/值张量，以提高KV缓存的内存利用率。通过将单一的键/值张量分解成更小的块，并将它们结构化到辅助前缀树中来实现这一点。在基于前缀树的KV缓存之上，我们设计了一个高效的自注意力核心，其中实现了一个两阶段分区算法，以在存在共享系统提示的情况下，改善自注意力计算中的数据局部性。\n\n#结果\n实验表明，与最先进的实现相比，ChunkAttention可以将自注意力核心的速度提高3.2-4.8倍，系统提示的长度从1024到4096不等。",
  "tldr": {
    "": "结果",
    "自注意力是大型语言模型的重要组成部分，但对于长序列而言，它是推理延迟的重要来源。在多租户大型语言模型服务场景中，通过利用多个LLM请求在前缀中共享系统提示的概率，可以优化自注意力的计算和内存操作成本。": "",
    "本文介绍了ChunkAttention，这是一个前缀感知的自注意力模块，能够在运行时检测多个请求中匹配的提示前缀，并在内存中共享它们的键/值张量，以提高KV缓存的内存利用率。通过将单一的键/值张量分解成更小的块，并将它们结构化到辅助前缀树中来实现这一点。在基于前缀树的KV缓存之上，我们设计了一个高效的自注意力核心，其中实现了一个两阶段分区算法，以在存在共享系统提示的情况下，改善自注意力计算中的数据局部性。": "",
    "实验表明，与最先进的实现相比，ChunkAttention可以将自注意力核心的速度提高3.2-4.8倍，系统提示的长度从1024到4096不等。": "",
    "动机": "",
    "方法": "",
    "结果": ""
  },
  "summary_cn": "自注意力是大型语言模型（LLMs）的一个基本组成部分，但对于长序列而言，它也是推理延迟的一个重要来源。在多租户LLMs服务场景中，通过使用多个LLM请求在前缀中共享系统提示的概率，可以优化自注意力的计算和内存操作成本。在本文中，我们介绍了ChunkAttention，这是一个前缀感知的自注意力模块，能够在多个请求中检测匹配的提示前缀，并在运行时在内存中共享它们的键/值张量，以提高KV缓存的内存利用率。这是通过将单一的键/值张量分解成更小的块，并将它们结构化到辅助前缀树中来实现的。因此，在基于前缀树的KV缓存之上，我们设计了一个高效的自注意力核心，其中实现了一个两阶段分区算法，以在存在共享系统提示的情况下，提高自注意力计算期间的数据局部性。实验表明，与最先进的实现相比，ChunkAttention可以将自注意力核心的速度提高3.2-4.8倍，系统提示的长度从1024到4096不等。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"Self-attention\",\n    \"Large Language Models\",\n    \"Inference Latency\",\n    \"Multi-tenant LLMs\",\n    \"Memory Optimization\",\n    \"ChunkAttention\",\n    \"Prefix-aware\",\n    \"Memory Utilization\",\n    \"KV Cache\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "Self-attention",
      "Large Language Models",
      "Inference Latency",
      "Multi-tenant LLMs",
      "Memory Optimization",
      "ChunkAttention",
      "Prefix-aware",
      "Memory Utilization",
      "KV Cache"
    ]
  }
}
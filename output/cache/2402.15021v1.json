{
  "id": "2402.15021v1",
  "raw_tldr": "动机\t近年来，视觉和语言任务的性能显著提升，但现有的基础视觉语言模型（如CLIP）在对象中心识别方面虽表现出色，却在学习文本表示时对词序不敏感，未能成功组合已知概念以创造新方式。\n方法\t本文介绍了一个框架，旨在显著提升现有模型编码组合语言的能力，同时保持或提升在标准对象识别和检索基准上的性能。\n结果\t该框架在组合性基准上实现了超过10%的绝对改进，同时在标准对象识别和检索基准上保持或提升了性能。",
  "tldr": {
    "动机": "近年来，视觉和语言任务的性能显著提升，但现有的基础视觉语言模型（如CLIP）在对象中心识别方面虽表现出色，却在学习文本表示时对词序不敏感，未能成功组合已知概念以创造新方式。",
    "方法": "本文介绍了一个框架，旨在显著提升现有模型编码组合语言的能力，同时保持或提升在标准对象识别和检索基准上的性能。",
    "结果": "该框架在组合性基准上实现了超过10%的绝对改进，同时在标准对象识别和检索基准上保持或提升了性能。"
  },
  "summary_cn": "近年来，视觉和语言任务的性能显著提升。基础视觉-语言模型（VLMs），如CLIP，在多个场景中被利用，并在多个任务中展现出了卓越的性能。这类模型擅长以物体为中心的识别，但学习到的文本表示似乎对词序不敏感，未能以新颖的方式组合已知概念。然而，没有证据表明任何VLM，包括大规模单流模型如GPT-4V，能成功识别组合。在本文中，我们介绍了一个框架，显著提高现有模型编码组合语言的能力，在组合性基准测试上有超过10%的绝对改进，同时在标准的物体识别和检索基准测试上保持或提高了性能。我们的代码和预训练模型在https://github.com/netflix/clove 上公开可用。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"多模态\",\n  \"标签\": [\"Vision-Language Models\", \"CLIP\", \"GPT-4V\", \"compositionality\", \"object-recognition\", \"retrieval benchmarks\"]\n}\n```",
  "tag_info": {
    "主要领域": "多模态",
    "标签": [
      "Vision-Language Models",
      "CLIP",
      "GPT-4V",
      "compositionality",
      "object-recognition",
      "retrieval benchmarks"
    ]
  }
}
{
  "id": "2402.17193v1",
  "raw_tldr": "动机\t尽管大型语言模型（LLMs）通常采用微调来释放其在下游应用中的能力，但我们对不同微调方法的归纳偏差（特别是规模化属性）的理解仍然有限。\n方法\t通过系统实验研究不同规模化因素（包括LLM模型大小、预训练数据大小、新微调参数大小和微调数据大小）如何影响微调性能，并考虑了全模型调整（FMT）和参数高效调整（PET，包括提示调整和LoRA）两种类型的微调，在数据有限的情况下探索它们的规模化行为。\n结果\tLLM微调遵循基于功率的乘法联合规模化法则，介于微调数据大小和每个其他规模化因素之间；LLM模型规模化的微调收益大于预训练数据规模化，而PET参数规模化通常无效；最佳微调方法高度依赖于任务和微调数据。",
  "tldr": {
    "动机": "尽管大型语言模型（LLMs）通常采用微调来释放其在下游应用中的能力，但我们对不同微调方法的归纳偏差（特别是规模化属性）的理解仍然有限。",
    "方法": "通过系统实验研究不同规模化因素（包括LLM模型大小、预训练数据大小、新微调参数大小和微调数据大小）如何影响微调性能，并考虑了全模型调整（FMT）和参数高效调整（PET，包括提示调整和LoRA）两种类型的微调，在数据有限的情况下探索它们的规模化行为。",
    "结果": "LLM微调遵循基于功率的乘法联合规模化法则，介于微调数据大小和每个其他规模化因素之间；LLM模型规模化的微调收益大于预训练数据规模化，而PET参数规模化通常无效；最佳微调方法高度依赖于任务和微调数据。"
  },
  "summary_cn": "尽管大型语言模型（LLMs）经常采用微调来释放其在下游应用中的能力，但我们对不同微调方法的归纳偏差（特别是规模化属性）的理解仍然有限。为了填补这一空白，我们进行了系统的实验，研究不同的规模化因素，包括LLM模型大小、预训练数据大小、新的微调参数大小和微调数据大小，如何以及是否影响微调性能。我们考虑了两种类型的微调——全模型调整（FMT）和参数高效调整（PET，包括提示调整和LoRA），并探索了它们在数据受限制的情况下的规模化行为，其中LLM模型大小显著超过微调数据大小。基于两套从1B到16B的预训练双语LLMs和在双语机器翻译及多语言摘要基准上的实验，我们发现：1）LLM微调遵循一个基于功率的乘法联合规模化法则，介于微调数据大小和每个其他规模化因素之间；2）LLM微调从LLM模型规模化中获益更多，而不是预训练数据规模化，且PET参数规模化通常无效；以及3）最佳的微调方法高度依赖于任务和微调数据。我们希望我们的发现能够帮助理解、选择和开发LLM微调方法。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"large language models\",\n    \"finetuning\",\n    \"inductive biases\",\n    \"scaling properties\",\n    \"full-model tuning\",\n    \"parameter efficient tuning\",\n    \"prompt tuning\",\n    \"LoRA\",\n    \"bilingual machine translation\",\n    \"multilingual summarization\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "large language models",
      "finetuning",
      "inductive biases",
      "scaling properties",
      "full-model tuning",
      "parameter efficient tuning",
      "prompt tuning",
      "LoRA",
      "bilingual machine translation",
      "multilingual summarization"
    ]
  }
}
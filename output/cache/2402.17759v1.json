{
  "id": "2402.17759v1",
  "raw_tldr": "动机\t这项工作旨在研究提高语言模型（LMs）学习的一般原则，目的是减少达到卓越性能所需的训练步骤。\n方法\t首先提出一个目标，通过最大化数据压缩比在“LM-training-as-lossless-compression”视角下优化LM学习。然后，推导出一个定理，名为学习定律，揭示了在我们的目标下最优学习过程中动态的属性。通过在线性分类和真实世界语言建模任务上的实验验证了该定理。\n结果\t实验验证了LMs的最优学习本质上源于LMs的缩放律系数的改进，表明为设计实用的学习加速方法提供了巨大的希望和意义。",
  "tldr": {
    "动机": "这项工作旨在研究提高语言模型（LMs）学习的一般原则，目的是减少达到卓越性能所需的训练步骤。",
    "方法": "首先提出一个目标，通过最大化数据压缩比在“LM-training-as-lossless-compression”视角下优化LM学习。然后，推导出一个定理，名为学习定律，揭示了在我们的目标下最优学习过程中动态的属性。通过在线性分类和真实世界语言建模任务上的实验验证了该定理。",
    "结果": "实验验证了LMs的最优学习本质上源于LMs的缩放律系数的改进，表明为设计实用的学习加速方法提供了巨大的希望和意义。"
  },
  "summary_cn": "本工作研究了改进语言模型（LMs）学习的一般原则，旨在减少实现卓越性能所需的训练步骤。具体来说，我们提出了一种优化LM学习的理论。我们首先提出了一个目标，通过最大化数据压缩比在“LM训练即无损压缩”的视角下优化LM学习。然后，我们推导出一个定理，名为学习定律，以揭示在我们的目标下最优学习过程中动态性的属性。该定理随后通过线性分类和一个真实世界语言建模任务的实验得到验证。最后，我们通过实证验证，LMs的最优学习本质上源于LMs的缩放律中系数的改进，这表明为设计实用的学习加速方法提供了巨大的希望和意义。我们的代码可以在https://aka.ms/LearningLaw找到。",
  "tag_info_raw": "```json\n{\n  \"主要领域\": \"NLP\",\n  \"标签\": [\n    \"language models\",\n    \"learning optimization\",\n    \"data compression\",\n    \"scaling law\",\n    \"learning acceleration\"\n  ]\n}\n```",
  "tag_info": {
    "主要领域": "NLP",
    "标签": [
      "language models",
      "learning optimization",
      "data compression",
      "scaling law",
      "learning acceleration"
    ]
  }
}